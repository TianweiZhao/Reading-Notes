{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28bbb9c9",
   "metadata": {},
   "source": [
    "# The Era of Human Data\n",
    "\n",
    "* While imitating humans is enough to reproduce many huamn capabilities to a competent level, this approach in isolation has not and likely cannot achieve superhuman intelligence across many important topics and tasks. \n",
    "* The pace of progress driven solely by supervised learning from human data is demonstrably slowing, signaling the need for a new approach. \n",
    "* Valuable new insights, such as new theorems, technologies or scientific breakthroughs, lie beyond the current boundaries of human understanding and cannot be captured by existing human data. \n",
    "\n",
    "# The Era of Experience\n",
    "* The data must be generated in a way that continually improves as the agent becomes stronger.\n",
    "* This can be achieved by allowing agents to learn continually from their own experience - data that is generated by the agent interacting with its environment. \n",
    "* Deepseek: \"underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problme, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies\" \n",
    "\n",
    "* The era of experience will likely be characterized by agents and environments that, in addition to learning from vast quantities of experiential data, will break through the limitations of human-centric AI systems in several further dimensions: \n",
    "    - Agents will inhabit streams of experience, rather than short snippets of interaction. \n",
    "    - Their actions and observations will be richly grounded in the environment, rather than interacting via human dialogue alone. \n",
    "    - Their rewards will be grounded in their experience of the environment, rather than coming from human prejedgement.\n",
    "    - They will plan and/or reason about experience, rather than reasoning solely in human terms. \n",
    "\n",
    "# Streams\n",
    "In the human era of human data, language-based AI has largely focused on short interaction episodes: e.g., a user asks a question and the agent responds. Typically, little or no information carries over from one episode to the next, precluding any adaptation over time. Furthermore, the agent aims exclusively for outcomes within the current episode, such as directly answering a user's question. _In contrast, humans exist in an ongoing stream of actions and observations that continues for many years. Information is carried across the entire stream, and their behavior adapts from past experiences to self-correct and improve._\n",
    "\n",
    "* The agent takes a sequence of steps so as to maximize long-term success with respect to the specified goal. An individual step may not provide any immediate benefit, or may even be detrimental in the short term, but may nevertheless contribute in aggregate to longer term success. This contrasts strongly with current AI systems that provide immediate responses to requests, without any ability to measure or optimize the future conseqeunces of their actions on the environment. \n",
    "\n",
    "# Actions and Observations\n",
    "* In the era of experience, agents will also interact with the real world via digital interfaces. For example, a scientific agent could monitor environmental sensors, remotely operate a telescope, or control a robotic arm in a lab to autonomously conduct experiments. \n",
    "\n",
    "# Rewards\n",
    "* What if experiential agents could learn from external events and signals, and not just human preferences?\n",
    "* The fact that these rewards or preferences are determined by humans in absence of their consequences, rather than measuring the effect of those actions on the environment, means that they are not directly grounded in the reality of the world. \n",
    "* Relying on human prejedgement in this manner usually leads to an impenetrable ceiling on the agent's performance: the agent cannot discover better strategies that are underappreciated by the humanrater. \n",
    "\n",
    "* Grounded rewards may arise from humans that are part of the agent's environment. \n",
    "* Where do rewards come from, if not from human data? Once agents become connected to the world through rich action and observation spaces, there will be no shortage of groundedd signals to provide a basis for reward. \n",
    "\n",
    "* There is an argument that even a single such reward signal, optimized with great effectivenss, may be sufficient to induce broadly capable intelligence -> This is because the achievement of simgple goal in a complex environment may often require a wide variety of skills to be mastered. \n",
    "\n",
    "* The idea is to flexibly adapt the reward, based on grounded signals, in a user-guided manner. For example, the reward function could be defined by a neural network that takes the agent's interactions with both the user and the environment as input, and outputs a scalar reward. This allows the reward to select or combine together signals from the environment in a manner that depends upon the user's goal. \n",
    "* Furthermore, users could provide feedback during the leraning process, such as their satisfaction level, which could be used to fine-tune the reward function. The reward function can then adapt over time, to improve the way in which it selects or combines signals, and to identify and correct any misalignment. \n",
    "* This can also be understood as a bi-level optimization process that optimizes user feedback as the top-level goal, and optimzes grounded signals from the environment at the low level. \n",
    "\n",
    "# Planning and Reasoning\n",
    "* In the era of human data, the reasoning methods have been explicitly designed to imitate human throught processes. The reasoning process may be fine-tuned further to produce thinking traces that match the correct answer, as determined by human experts. \n",
    "* However, it is highly unlikely that human language provides the optimal instance of a universal computer. More efficient mechanisms of thought surely exist, using non-human languages that may for example utilize symbolic, distributed, continuous, or differentialble computations. **A self-learning system can in principle discover or improve such approaches by learning how to think from experience**. \n",
    "* An agent trained to imitate human thoughts or even to match human expert answers may inherit fallacious methods of thought deeply embedded within that data, such as flawed assumptions or inherent biases. \n",
    "* The grounding provides a feedback loop, allowing the agent to test its inherited assumptions against reality and discover new principles that are not limited by current dominant modes of human thought. Without this grounding, an agent, no matter how sophisticated, will become an echo chamber of existing human knowledge. \n",
    "* _To move beyond this, agents much actively engage with the world, collect observational data, and use that data to iteratively refine their understanding._\n",
    "* As the agent continues to interact with the worlds throughout its stream of experience, its dynamics model is continually updated to correct any errors in its predictions. Given a world model, an agent may apply scalable planning methods taht improve the predicted performance of the agent. \n",
    "\n",
    "\n",
    "# RL Methods\n",
    "* While human-centric RL has enabled an unprecedented breadth of behaviors, it has also imposed a new ceiling on the agent's performance: agents cannot go beyond existing human knowledge. \n",
    "* The era of experience presents an opportunity to revisit and improve classic RL concepts. \n",
    "\n",
    "# Consequences\n",
    "* positive side: experiential learning will unlock unprecedented capabilities. \n",
    "* challenge: Moving away from human data and human modes of thinking may also make future AI systems harder to interpret.\n",
    "* An experiential agent is aware of the envirnoment it is suited within, and its behavior can adapt over time to changes in that environment. \n",
    "* The misaligned rewrad functions can often be incrementally corrected over time by trial and error. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
