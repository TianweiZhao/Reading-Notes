{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9ea10b",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "* Key challenge: reward estimation during inference while not having access to ground-truth information. \n",
    "\n",
    "# 2. TTRL\n",
    "\n",
    "* Unlike traditional RL, where the agent learns from known reward signals, TTRL operates on unlabeled test data. In other words, the model must learn and adapt without access to explicit supervision. \n",
    "* \"We study the problem of training a pre-trained model during test time using RL without ground-truth labels. We call this setting Test-Time RL\"\n",
    "\n",
    "* The approach enables the model to adapt during inference, effectively improving its performance on distribution-shifted inputs without the need for labeled data. \n",
    "\n",
    "* Why does TTRL work? \n",
    "\n",
    "1. Label estimation\n",
    "    - A direct difference between TTRL and standard RL algorithms is that TTRL involves label estimation, which introduces reward inaccuracies. TTRL works despite these inaccuracies due to the following two reasons:\n",
    "    1) existing studies have shown that RL can tolerate a certain degree of reward inaccuracy. Moreover, RL tends to generalize better than supervised fine-tuning (SFT), which often relies on memorizing training data. In RL, rewards are typically vague AND SERVE PRIMARILY AS DIRECTIONAL SIGNALS for exploration, leading to RL's robustness to reward noise. \n",
    "    2) More accurate reward models are not necessarily better teachers. Therefore, rewrad signals estimated by the policy model itself may offer more suitable guidance for learning. \n",
    "\n",
    "2. Reward calculations: \n",
    "    - When the model is capable of estimating accurate labels via majority voting, the subsequently estimated rewrad is generally reliable. However, _why does TTRL remain effective even when the model fails to estimate accurate lables on challenging benchmarks? The most fundamental reason lies in the definition of rewards in RL. \n",
    "    - Rule-based rewards are assigned based on whether the predicted answer matches the \"label\". Therefore, even if an estimateed label is not the ground-truth, as long as it differs from an incorrectly predicted asnwer, the system can still assgin a correct \"negative\" reward. \n",
    "\n",
    "\n",
    "# 3. Test-time scaling:\n",
    "* TTS is designed to enhance the capabilities of LLMs in handling complex tasks by increasing computational resources at test time. Reallocating the same computational resources from pre-training to test-time could yield greater improvements in model performance.\n",
    "\n",
    "# Conclusion: \n",
    "A key component of TTRL is its majority voting reward function, which generates rule-based rewards based on consensus among model predictions. View TTRL as a preliminary step toward RL with self-labeled rewards, marking an important direction of learning from continuous streams of experience. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
