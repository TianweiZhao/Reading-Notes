{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context: Delays and Overshoot in Real-World CSTR Operations\n",
    "\n",
    "* Actuator Deadtime:\n",
    "    Opening or closing a valve (e.g., adjusting $F_{in}$) might not be instantaneous. The valve might take a few seconds or minutes to fully respond to the control signal.\n",
    "\n",
    "* Process Deadtime:\n",
    "    Chemical reactions, especially in large reactors, often have transport and reaction delays. The concentration or temperature measurement might only reflect a control action taken some time ago.\n",
    "\n",
    "* Consequence:\n",
    "    Without accounting for these delays, a typical RL agent (or standard PID) might keep incerasing the jacket temperature ($T_c$) because it doesn't immediately see the effect of its action. By the time the effect becomes apparent, it might have overshot the setpoint significantly, then it reacts aggresively in the other direction, causing osciallations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Incorporating Time-Delay Awareness into RL\n",
    "\n",
    "## 1.1 LSTM in Actor/Critic Netowrks\n",
    "\n",
    "1. Motivation:\n",
    "    * An LSTM (Long Short-Term Memory) layer captures temporal dependencies and delayed cause-and-effect relationships in the system.\n",
    "    * By storing recent history of states (and potentially actions), the RL agent can infer when the system is still responding to a prior adjustment.\n",
    "\n",
    "2. Mechanics:\n",
    "    * Actor Network:\n",
    "        * Instead of Actor ($s_t$) being a simple feedback netowkr, make it ActorLSTM({$s_t, s_{t-1}, ...$})\n",
    "        * The LSTM's hidden state effectively tracks delayed system responses, enabling the policy to anticipate overshoot. \n",
    "    * Critic Network:\n",
    "        * Similarly incorportate LSTM in Critic ($s_t, a_t$)\n",
    "        * The critic learns that an action taken at time $t - \\Delta$ might only show up in the measured concentration at time $t$.\n",
    "\n",
    "3. Reward shaping:\n",
    "    * Provide negative rewards for large overshoot or high control action rates\n",
    "    * The LSTM helps the agent realize it should wait for the reaction to \"catch up\" rather than continuing to push the same control input.\n",
    "\n",
    "\n",
    "## 1.2 Prediction-Based or Model-Based Approaches:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Practical Solutions for Deadtime and Overshoot\n",
    "\n",
    "1. Deadtime-Aware States:\n",
    "    The RL state could include delayed actions or a time-shifted buffer of past actions. For exapmle, if we know the valve opening will appear in the measurement after 3 time units, the RL agent sees both the intended action from time $t$ and the actual effect only at $t + 3$\n",
    "\n",
    "2. Delayed observations:\n",
    "    If the environment does not show changes in temperature or concentration until some steps later, the LSTM can store the prior actions and states so the agent can infer, \"We are still waiting on the effect of the last big temperature increase.\"\n",
    "\n",
    "3. Overshoot Penalization:\n",
    "    * Enhance the reward function:\n",
    "    $$\n",
    "    r_t = -\\alpha (\\text{tracking\\_error}_t)^2 - \\beta (\\text{control\\_effort}_t)^2 - \\gamma \\max(0, \\text{overshoot}_t)\n",
    "    $$\n",
    "    * Where the overshoot$_t$ is the positive difference if the measurement goes above the setpoint. This explicitly teaches the agent to avoid large overshoot. \n",
    "\n",
    "4. Adaptive Critic Frequency + Delay Buffer\n",
    "    * Combine the idea of adaptive critic frequency with a delay buffer inside the environment\n",
    "    * Each time the agent acts, the environment internally queues the effect of that action to apply it $\\tau$ steps later, simulating deadtime\n",
    "    * The agent's LSTM then accounts for how older actions might be just now taking effect\n",
    "\n",
    "5. Decision Tree Branches for Delay:\n",
    "    * In the decision-tree approach, certain branches might specifically handle large known delays, for instance\n",
    "        * Branch condition: \"If current time < (last_setpoint_change_time + deadtime_constant), expect delayed effect\".\n",
    "        * This branch might instruct the RL agent (or the hierarchical sub-policy) to moderate changes\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Illustrative Extended Pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM-Enhanced Actor for Deadtime Handling\n",
    "class LSTMActorNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, seq_inputs):\n",
    "        # seq_inputs: shape (batch, time_seq, input_dim)\n",
    "        output, (h_n, c_n) = self.lstm(seq_inputs)\n",
    "        # take the last time step hidden state\n",
    "        last_output = output[:, -1, :]\n",
    "        action = torch.tanh(self.fc(last_output))  # ensure [-1,1] for PID gains\n",
    "        return action\n",
    "\n",
    "# Environment with Delayed Action Effect\n",
    "class DelayedCSTREnvironment:\n",
    "    def __init__(self, deadtime_steps=2, ...):\n",
    "        self.deadtime_steps = deadtime_steps\n",
    "        self.action_queue = collections.deque(maxlen=deadtime_steps+1)\n",
    "        ...\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Store the new action in a queue\n",
    "        self.action_queue.append(action)\n",
    "        \n",
    "        # Apply the oldest action from the queue if enough time has passed\n",
    "        if len(self.action_queue) >= self.deadtime_steps:\n",
    "            actual_action = self.action_queue[0]\n",
    "        else:\n",
    "            actual_action = np.zeros_like(action)  # or default action\n",
    "        # Next, simulate environment with 'actual_action'\n",
    "        next_state, reward, done, info = cstr_simulation(actual_action)\n",
    "        ...\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "# Adaptive RL Training with LSTM and Delayed Environment\n",
    "for episode in range(num_episodes):\n",
    "    seq_state = []\n",
    "    seq_action = []\n",
    "    state = delayed_env.reset()\n",
    "    hidden_lstm = actor.init_hidden()  # if needed\n",
    "    \n",
    "    while not done:\n",
    "        # Collect temporal inputs for LSTM\n",
    "        seq_state.append(state)\n",
    "        \n",
    "        # If we have enough history, feed to LSTM\n",
    "        if len(seq_state) >= seq_length:\n",
    "            batch_seq = format_batch(seq_state, ...\n",
    "            action = actor(batch_seq)\n",
    "            \n",
    "            # Evaluate environment\n",
    "            next_state, reward, done, info = delayed_env.step(action)\n",
    "            \n",
    "            # Manage overshoot penalty, error derivative\n",
    "            reward += compute_overshoot_penalty(next_state)\n",
    "            \n",
    "            # Critic update possibly with multi-timescale logic\n",
    "            # ...\n",
    "            \n",
    "            seq_state.pop(0)  # remove oldest\n",
    "        else:\n",
    "            # If not enough history, pick an initial action\n",
    "            action = np.zeros(action_dim)\n",
    "            next_state, reward, done, info = delayed_env.step(action)\n",
    "        \n",
    "        state = next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Benefits of This Enhanced Design\n",
    "\n",
    "1. Overshoot Reduction:\n",
    "    Because the LSTM \"remembers\" previous big changes in temperature/flow, the agent learns to wait for delayed effects before continuing to push the same control variable. \n",
    "2. More Realistic Control:\n",
    "    The environment's step function includes a queue for delayed actions, mimicking real-world slow valve (or other action) responses. \n",
    "3. Intrepretability Remains Feasible:\n",
    "    The decision tree or \"supervisory strucutre\" can still provide rule-based logic about operating regimes.\n",
    "    The LSTM handles the more subtle aspects of delayed cause-and-effect\n",
    "4. Ease of Transfer to Real Plant\n",
    "    Modeling the deadtime in simulation means the policy is more likely to handle real delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another wild idea: LSTM + Kalman Filter enhanced RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overall Architecture\n",
    "\n",
    "This system combines three key components:\n",
    "\n",
    "* **Kalman Filter**: A model-based estimation that refines noisy measurements from the nonlinear CSTR system\n",
    "* **LSTM network**: A recurrent neural network that takes a sequence of filtered state estimates and extracts temporal features, providing an enhanced state representation\n",
    "* **Reinforcement Learning**: Using a Soft Actor Critic framework, the agent learns to output PID controller tuning parameters (six gains) based on the enhanced state\n",
    "\n",
    "Together, these components work in tandem to adaptively tune the PID controller for a nonlinear system with noise and deadtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Detailed Implementation Walkthrough\n",
    "\n",
    "a. Kalman filter:\n",
    "* Purpose:\n",
    "    The Kalman filter is used to perform state estimation despite noisy sensor measurements. In a nonlinear system like the CSTR, even though the Kalman filter assumes linear dynamics, it can still provide a good approximation for noise reduction. \n",
    "\n",
    "* Implementation Details:\n",
    "    * Initialization: The filter starts with an initial state vector (x_hat) and covariance (P)\n",
    "    * Predict Step: Uses the state transition matrix (F) and process noise covariance (Q) to predict the next state\n",
    "    * Update Step: When a new measurement arrives, it computes the innovation (residual between measurement and prediction), calculates the Kalman gain, and updates both the state estimate and covariance\n",
    "    * Customization: You have an option to extract state variables (e.g., concentration, temperature, volume) from the full-state vector to focus the estimation on the most relevant aspects.\n",
    "\n",
    "b. LSTM State Enhancer\n",
    "* Purpose:\n",
    "    The LSTM network processes a sliding window of state estimates (produced by the Kalman filter) to capture time-dependent patterns or trends. This is particularly important in systems with delays or deadtime, as the current state might depend on past states.\n",
    "* Implementation Details: \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
