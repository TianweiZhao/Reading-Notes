{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "618b03a8",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "* One challenge DRL faces is that the full state of the system is often not observable. When this is the case, the policy needs to leverage the history of observations to infer the current state. At the same time, differences between the training and testing environments makes it critical for the policy not to overfit to the sequence of observations it sees at training time. \n",
    "* As such, there is an important balancing act between ahving the history encoder be flexible enough to extract relevant information, yet be robust to changes in the environment. \n",
    "\n",
    "# Introduction\n",
    "* Partially Observable Markov Decision Process (POMDP)\n",
    "* another hurdle stems from the fact that the policy are often trained in an imperfect simulator, which is likely different from the true environment. \n",
    "* Challenge: find a balance between extracting useful information from the history and avoiding overfitting to modelling error. \n",
    "* The use of recurrent network architectures in DRL for POMDPs was one of the initial proposed solutions and remains a prominent approach for control tasks. \n",
    "\n",
    "# Preliminaries\n",
    "* In a Partially Observable Markov Decision Process (POMDP), the observations that the policy receives are not the true states of the process. In control this may happen for a variety of reasons such as noisy observations made by sensors, but in this work we specifically focus on the case where aspects of the state space remain unmeasured. \n",
    "* The objective remains the same as the MDP, but now the policy and value functions are not allowed access to the state. \n",
    "* PID controllers are designed for SISo control problems, but many real-world systems are MIMO. In the case of MIMO tracking problems, wehere there are M signals with M corresponding actuators, one can control the system with M separate PID controllesr. \n",
    "* However, this assumes there is a clear breakdown of which actuator influences which signal. Additioanlly, there are often interactions betweent he different signals, which the PID controllers do not account for. \n",
    "* Beyond tracking problems, it is less claer how to use PID controllers without substantial engineering efforts. \n",
    "\n",
    "# Methodology\n",
    "\n",
    "* How should one choose a historical encoder with these challenges in mind? \n",
    "    - LSTM\n",
    "    - Gated Recurrent Units\n",
    "    - Transformers\n",
    "* In reality, the extra flexibility of these architectures may become a hindrance when deployed on the physical device if they overfit to quirks in the simulator. \n",
    "\n",
    "# Related Work\n",
    "* A control task may be partially observable for a myriad of reasons including unmeasured state variables, sensor noise, and unmeasured system parameters. When there are unmeasured system parameters, this is usually framed as a meta-reinforcement learning (MetaRL) problem. \n",
    "* this is a specific subclass of POMDPs where there is a collection of MDPs and each episode, an MDP is sampled from this collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552169ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
