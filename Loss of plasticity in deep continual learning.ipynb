{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe621fed",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "* Learning these networks refers to computational algorithms for changing the strengths of their connection weights (computational synapses). The most important modern learning methods are based on stochastic gradient descent (SGD) and the backpropagation algorithm. \n",
    "* Despite its successes, deep learning has difficulty adapting to changing data. Because of this, deep learning is restricted to a special training phase and then turned off when the network is actually used. \n",
    "* In practice, the most common strategy for incorporating substantial new data has been simply to discard the old network and train a new one from scratch on the old and new data together. \n",
    "* Change is ubiquitous in learning to anticipate markets and human preferences and in gaming, logistics and control systems. \n",
    "* Loss of plasticity: deep learning methods lose their ability to learn with extended training on new data. \n",
    "* Continual backpropagation: exactly like backpropagation except that a tiny proportion of less-used units are reinitialized on each step much as they were all initialized at the start of training. \n",
    "\n",
    "### Plasticity loss in RL\n",
    "\n",
    "* Not only can the environment change but the behavior of the learning agent can also change, thereby influencing the data it receives even if the environment is stationary. For this reason, the need for continual learning is often more apparent in RL, and RL is an important setting in which to demonstrate the tendency of deep learning towards loss of plasticity. \n",
    "\n",
    "* The learning ability of an algorithm is thus confounded with its ability to generate informative data. \n",
    "\n",
    "* Results show that plasticity loss can be catastrophic in both deep reinforcement learning as well as deep supervised learning. \n",
    "\n",
    "### Maintaining plasticity\n",
    "\n",
    "* Surprisingly, popular methods such as Adam, Dropout and normalization actually increased loss of plasticity. L2 regularization, on the other hand, reduced loss of plasticity in many cases. L2 regularization stops the weights from becoming too large by moving them towards zero at each step. The small weights allow the network to remain plastic. \n",
    "* The injection of variability into the network can reduce dormancy and increase the diversity of the representation. \n",
    "* Non growing weights and sustained variability in the network may be important for maintaining plasticity. \n",
    "* Conventional backpropagations has two main parts: initialization with small random weights before training and then gradient descent at each training step. The initialization provides variability initially but with continued learning, variability tends to be lost, as well as plasticity along with it. \n",
    "* To maintain the variability, continual backpropagation reinitializes a small number of units during training, typically fewer than one per step. To prevent disruption of what the network has already learned, only the least-used units are considered for reinitialization. \n",
    "\n",
    "\n",
    "### Discussion\n",
    "\n",
    "* In settings in which learning must continue, however, we have shown that deep learning does not work. By deep learning, we mean the existing standard algorithms for learning in multilayer artificial neural networks and by not work, we mean that, over time, they fail to learn appreciably better than shallow networks. \n",
    "* During training, many of the network's neuron-like units become dormant, overcommitted and similar to each other, hampering the ability of the networks to learn new things. As they learn, standard deep-learning networks gradually and irreversibly lose their diversity and thus their ability to continue leraning. \n",
    "* Plasticity loss is often severe when learning continues for many tasks, but may not occur at all for small number of tasks. \n",
    "* Continual backpropagation involves a form of variation and selection in the space of neuron-like units, combined with continuing gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5192c21",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
