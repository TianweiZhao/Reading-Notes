{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deba6b7d-4015-4bbf-9061-845a5ed2dfeb",
   "metadata": {},
   "source": [
    "# What is the Actor-Critic Algorithm\n",
    "\n",
    "The actor-critic algorithm is a type of reinforcement learning algorithm that combines aspects of both **policy-based methods (Actor)** and **value-based methods (Critic)**. This hybrid approach is designed to address the limitations of each method when used individually. \n",
    "\n",
    "In the actor-critic framework, an agent (the \"actor\") learns a policy to make decisions, and a value function (the \"critic\") evaluates the actions taken by the actor. \n",
    "\n",
    "Simultaneously, the critic evaluates these actions by estimating their value or quality. This dual role allows the method to strike a balance between exploration and exploitation, leveraging the strengths of both policy and value functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a3cdd1-a5fc-4f99-b68a-e8f4b78554f3",
   "metadata": {},
   "source": [
    "# Roles of Actor and Critic\n",
    "\n",
    "* Actor: The actor makes decisions by selecting actions based on the current policy. Its responsibility lies in exploring the action space to maximize expected cumulative rewards. By continuously refining the policy, the actor adapts to the dynamic nature of the environment.\n",
    "\n",
    "* Critic: The critic evaluates the actions taken by the actor. It estimates the value or quality of these actions by providing feedback on their performance. The critic's role is pivotal in guiding the actor towards actions that lead to hgiher expected returns, contributing to the overall improvement of the learning process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f21b740-d1b1-44f4-ac93-8918a3a0cc0d",
   "metadata": {},
   "source": [
    "# Key Terms in Actor Critic Algorithm\n",
    "\n",
    "* Policy (Actor):\n",
    "  * The policy, denoted as $\\pi(a|s)$, represents the probability of taking action **a** in state **s**.\n",
    "  * The actor seeks to maximize the expected return by optimizing this policy.\n",
    "  * The policy is modeled by the actor network, and its parameters are denoted by $\\theta$.\n",
    " \n",
    "* Value Function (Critic):\n",
    "  * The value function, denoted as $V(s)$, estimates the expected cumulative reward starting from state **s**.\n",
    "  * The value function is modeled by the critic network, and its parameters are denoted by **w**.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b1da01-7692-461d-8017-b9a4b81e9f86",
   "metadata": {},
   "source": [
    "# How Actor-Critic Algorithm works? \n",
    "\n",
    "## Actor Critic Algorithm Objective Function\n",
    "* The objective function for the Actor-Critic algorithm is a combination of the policy gradient (for the actor) and the value function (for the critic).\n",
    "* The overall objective function is typically expressed as the sum of two components:\n",
    "\n",
    "**Policy Gradient (Actor)**\n",
    "\n",
    "$$\\nabla_{\\theta}J(\\theta)\\approx \\frac{1}{N}\\sum_{i=0}^N \\nabla_{\\theta}\\log\\pi_{\\theta}(a_i|s_i)\\cdot A(s_i, a_i)$$\n",
    "\n",
    "Here,\n",
    "* $J(\\theta)$ represents the expected return under the policy parameterized by $\\theta$\n",
    "* $\\pi_{\\theta}(a|s)$ is the policy function\n",
    "* N is the number of sampled experiences\n",
    "* $A(s,a)$ is the advantage function representing the advantage of taking action a is state s\n",
    "* $i$ represents the index of the sample\n",
    "\n",
    "\n",
    "**Value Function Update (Critic)**\n",
    "\n",
    "$$\\nabla_{w}J(w)\\approx\\frac{1}{N}\\sum_{i=1}^N \\nabla_{w} (V_w(s_i)-Q_w(s_i,a_i))^2$$\n",
    "\n",
    "Here,\n",
    "* $\\nabla_{w}J(w)$ is the gradient of the loss function with respect to the critic's parameters $w$\n",
    "* N is the number of samples\n",
    "* $V_w(s_i)$ is the critic's estimate of value of state s with parameter w\n",
    "* $Q_w(s_i, a_i)$ is the critic's estimate of the action value of taking action a\n",
    "* $i$ represents the index of the sample\n",
    "\n",
    "\n",
    "## Update Rules\n",
    "The update rules for the actor and critic involve adjusting their respective parameters using gradient ascent (for the actor) and gradient descent (for the critic).\n",
    "\n",
    "**Actor Update**\n",
    "\n",
    "$$\\theta_{t+1}=\\theta_t + \\alpha\\nabla_{theta}J(\\theta_{t})$$\n",
    "\n",
    "Here,\n",
    "* $\\alpha$: learning rate for the actor\n",
    "* t is the time step within an episode\n",
    "\n",
    "**Critic Update**\n",
    "\n",
    "$$w_t = w_t - \\beta \\nabla_{w}J(w_t)$$\n",
    "\n",
    "Here,\n",
    "* w represents the parameters of the critic network\n",
    "* $\\beta$ is the learning rate for the critic\n",
    "\n",
    "## Advantage Function\n",
    "\n",
    "The advantage function, $A(s,a)$, measures the advantage of taking action a in state s over the expected value of the state under the current policy.\n",
    "\n",
    "$$A(s,a) = Q(s,a) - V(s)$$\n",
    "\n",
    "The advantage function, then, provides a measure of how much better or worse an action is compared to the average action. The actor is updated based on the policy gradient, encouraging actions with hgiher advantages, while the critic is updated to minimize the difference between the estimated value and the action-value. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b86915-7251-4679-995d-245c6e81a811",
   "metadata": {},
   "source": [
    "# A2C (advantage Actor Critic)\n",
    "\n",
    "A2C is a specific variant of the actor-critic algorithm that introduces the concept of the **advantage function**. This function measures how much better an action is compared to the average action in a given state. By incorporating this advantage information, A2C focuses the learning process on actions that have a significantly higher value than the typical action taken in that state.\n",
    "\n",
    "While both leverage the actor-critic architecture, here's a key distinction between them:\n",
    "* Learning from the Average: The base Actor Critic method uses the difference between the actual reward and the estimated value (critic's evaluation) to update the actor.\n",
    "* Learning from the Advantage: A2C leverages the advantage function, incorporating the difference between the action's value and the average value of actions in that state. This additional information refines the learning process further.\n",
    "\n",
    "## Actor-Critic Algorithm Steps\n",
    "\n",
    "The Actor-Critic algorithm combines these mathematical principles into a coherent learning framework. The algorithm involves:\n",
    "1. Initialization: Initialize the policy parameters $\\theta(actor)$ and the value function parameters $\\phi(critic)$.\n",
    "2. Interaction with the Environment: The agent interacts with the environment by taking actions according to the current policy and receiving observations and rewards in return.\n",
    "3. Advantage Computation: Compute the advantage function $A(s,a)$ based on the current policy and value estimates.\n",
    "4. Policy and Value Updates:\n",
    "   *  Simultaneously update the actor's parameters $(\\theta)$ using the policy gradient. The policy gradient is derived from the advantage function and guides the actor to increase the probabilities of actions that lead to higher advantages.\n",
    "   *  Simultaneously update the critic's parameters $(\\phi)$ using a value-based method. This often involves minimizing the temporal difference (TD) error, which is the difference between the observed rewards and the predicted values.\n",
    "  \n",
    "The actor learns a policy, and the critic evaluates the action taken by the actor. The actor is updated using the policy gradient, and the critic is updated using a value-based method. This combination allows for more stable and efficient learning in complex environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa5c43-ff6c-493c-837a-35254bd374f7",
   "metadata": {},
   "source": [
    "# Training Agent: Actor-Critic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e1c11c-4162-47bc-b49c-0fbb5afedadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542c5b1c-4196-41cc-847e-0142c09f7cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating CartPole environment\n",
    "\n",
    "env = gym.make('CartPole-v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5718306-d466-46aa-917b-68e36ced29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Actor and Critic Networks\n",
    "# Actor and the Critic are implemented as neural networks using TensorFlow's Keras API\n",
    "# Actor network maps the state to a probability distribution over actions.\n",
    "# Critic network estimates the state's value\n",
    "\n",
    "actor = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(env.action_space.n, activation='softmax')\n",
    "])\n",
    "\n",
    "critic = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03b3b849-02a9-4e2e-894b-3ce7ed143b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss functions\n",
    "\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ff7903-0ef2-48b3-8585-79ab9a1625cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "Episode 0, Reward: 24.0\n",
      "Episode 10, Reward: 27.0\n",
      "Episode 20, Reward: 27.0\n",
      "Episode 30, Reward: 19.0\n",
      "Episode 40, Reward: 17.0\n"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "num_episodes = 50\n",
    "gamma = 0.99\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, info = env.reset() # for each episode, it resets the environment and initializes the episode reward to 0\n",
    "    state = obs.flatten()\n",
    "    episode_reward = 0\n",
    "\n",
    "    # compute gradients for the acrtor and critic networks\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        for t in range(1, 10000):  # Limit the number of time steps\n",
    "            # Choose an action using the actor\n",
    "            # agent chooses an action based on the actor's output probabilities and takes that action in the environment\n",
    "            action_probs = actor(np.array([state]))\n",
    "            action = np.random.choice(env.action_space.n, p=action_probs.numpy()[0])\n",
    "\n",
    "            # Take the chosen action and observe the next state and reward\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "\n",
    "            # Compute the advantage\n",
    "            # advantage function is the difference between the expected return \n",
    "            # and the estimated value at the current state\n",
    "            state_value = critic(np.array([state]))[0, 0]\n",
    "            next_state_value = critic(np.array([next_state]))[0, 0]\n",
    "            advantage = reward + gamma * next_state_value - state_value\n",
    "\n",
    "            # Compute actor and critic losses based on advantage function\n",
    "            actor_loss = -tf.math.log(action_probs[0, action]) * advantage\n",
    "            critic_loss = tf.square(advantage)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update actor and critic\n",
    "            # Gradients are computed using tape.gradient\n",
    "            # then applied to update the actor and critic networks using the respective optimizers\n",
    "            actor_gradients = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "            critic_gradients = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "            actor_optimizer.apply_gradients(zip(actor_gradients, actor.trainable_variables))\n",
    "            critic_optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "    # every 10 episodes, the current episode number and reward are printed\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e54dc0e-4160-48ef-a74d-695d98363168",
   "metadata": {},
   "source": [
    "# Advantages of Actor Critic Algorithm\n",
    "\n",
    "1. **Improved Sample Efficiency**: The hybrid nature of actor-critic algorithms often leads to improved sample efficiency, requiring fewer interactions with the environment to achieve optimal performance.\n",
    "\n",
    "2. **Faster Convergence**: The method's ability to update both the policy and value function concurrently contributes to faster convergence during training, enabling quicker adaptation to the learning task.\n",
    "\n",
    "3. **Versatility Across Action Spaces**: Actor-Critic architectures can seamlessly handle both discrete and continuous action spaces, offering flexibility in addressing a wide range of RL problems.\n",
    "\n",
    "4. **Off-Policy Learning (in some variants)**:Learns from past experiences, even when not directly following the current policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db5af3e-c42a-4655-b173-259dd014cb03",
   "metadata": {},
   "source": [
    "# Advantage Actor Critic (A2C) vs. Asynchronous Advantage Actor Critic (A3C)\n",
    "\n",
    "Asynchronous Advantage Actor-Critic (A3C) builds upon A2C by introducing parallelism.\r\n",
    "In A2C, a **single actor-critic pair** interacts with the environment and updates its policy based on the experiences it gathers. However, A3C utilizes **multiple actor-critic pairs** operating simultaneously. Each pair interacts with a separate copy of the environment, collecting data independently. These experiences are then used to update a global actor-critic network.\n",
    "\n",
    "Imagine **training multiple agents simultaneously**, **each exploring a separate world**. That's the core idea behind A3C (Asynchronous Advantage Actor-Critic). These agents, called \"workers,\" independently learn from their experiences and update a central value function. This parallel approach allows A3C to explore the environment much faster than a single agent, leading to quicker learning.\n",
    "\n",
    "A2C (Advantage Actor-Critic) is like A3C's simpler cousin. It uses the same core concept of actor-critic with an advantage function, but without the parallel workers. While A2C explores the environment less extensively, studies have shown it can achieve similar performance to A3C while being easier to implement and requiring less computational power.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa62760-2045-4af1-8555-f07cc87c0ac3",
   "metadata": {},
   "source": [
    "## RL (A3C) using Pytorch + multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecbd1591-940d-45a5-b2d9-3511c70cd411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def v_wrap(np_array, dtype=np.float32):\n",
    "    if np_array.dtype != dtype:\n",
    "        np_array = np_array.astype(dtype)\n",
    "    return torch.from_numpy(np_array)\n",
    "\n",
    "\n",
    "def set_init(layers):\n",
    "    for layer in layers:\n",
    "        nn.init.normal_(layer.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(layer.bias, 0.)\n",
    "\n",
    "\n",
    "def push_and_pull(opt, lnet, gnet, done, s_, bs, ba, br, gamma):\n",
    "    if done:\n",
    "        v_s_ = 0.               # terminal\n",
    "    else:\n",
    "        v_s_ = lnet.forward(v_wrap(s_[None, :]))[-1].data.numpy()[0, 0]\n",
    "\n",
    "    buffer_v_target = []\n",
    "    for r in br[::-1]:    # reverse buffer r\n",
    "        v_s_ = r + gamma * v_s_\n",
    "        buffer_v_target.append(v_s_)\n",
    "    buffer_v_target.reverse()\n",
    "\n",
    "    loss = lnet.loss_func(\n",
    "        v_wrap(np.vstack(bs)),\n",
    "        v_wrap(np.array(ba), dtype=np.int64) if ba[0].dtype == np.int64 else v_wrap(np.vstack(ba)),\n",
    "        v_wrap(np.array(buffer_v_target)[:, None]))\n",
    "\n",
    "    # calculate local gradients and push local parameters to global\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    for lp, gp in zip(lnet.parameters(), gnet.parameters()):\n",
    "        gp._grad = lp.grad\n",
    "    opt.step()\n",
    "\n",
    "    # pull global parameters\n",
    "    lnet.load_state_dict(gnet.state_dict())\n",
    "\n",
    "\n",
    "def record(global_ep, global_ep_r, ep_r, res_queue, name):\n",
    "    with global_ep.get_lock():\n",
    "        global_ep.value += 1\n",
    "    with global_ep_r.get_lock():\n",
    "        if global_ep_r.value == 0.:\n",
    "            global_ep_r.value = ep_r\n",
    "        else:\n",
    "            global_ep_r.value = global_ep_r.value * 0.99 + ep_r * 0.01\n",
    "    res_queue.put(global_ep_r.value)\n",
    "    print(\n",
    "        name,\n",
    "        \"Ep:\", global_ep.value,\n",
    "        \"| Ep_r: %.0f\" % global_ep_r.value,\n",
    "    )\n",
    "\n",
    "\n",
    "class SharedAdam(torch.optim.Adam):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        super(SharedAdam, self).__init__(params, lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        # State initialization\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = 0\n",
    "                state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                # share in memory\n",
    "                state['exp_avg'].share_memory_()\n",
    "                state['exp_avg_sq'].share_memory_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c4af3d-111b-4695-9bc7-e11663468b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from utils import v_wrap, set_init, push_and_pull, record\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "# from shared_adam import SharedAdam\n",
    "import gymnasium as gym\n",
    "import math, os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c516ae71-2e30-4f8b-85c9-6bc8d9b24042",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_GLOBAL_ITER = 5\n",
    "GAMMA = 0.9\n",
    "MAX_EP = 300\n",
    "MAX_EP_STEP = 20\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "N_S = env.observation_space.shape[0]\n",
    "N_A = env.action_space.shape[0]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.a1 = nn.Linear(s_dim, 200)\n",
    "        self.mu = nn.Linear(200, a_dim)\n",
    "        self.sigma = nn.Linear(200, a_dim)\n",
    "        self.c1 = nn.Linear(s_dim, 100)\n",
    "        self.v = nn.Linear(100, 1)\n",
    "        set_init([self.a1, self.mu, self.sigma, self.c1, self.v])\n",
    "        self.distribution = torch.distributions.Normal\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = F.relu6(self.a1(x))\n",
    "        mu = 2 * F.tanh(self.mu(a1))\n",
    "        sigma = F.softplus(self.sigma(a1)) + 0.001 \n",
    "        c1 = F.relu6(self.c1(x))\n",
    "        values = self.v(c1)\n",
    "        return mu, sigma, values\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        self.training = False\n",
    "        mu, sigma, _ = self.forward(s)\n",
    "        m = self.distribution(mu.view(1, ).data, sigma.view(1, ).data)\n",
    "        return m.sample().numpy()\n",
    "\n",
    "    def loss_function(self, s, a, v_t):\n",
    "        self.train()\n",
    "        mu, sigma, values = self.forward(s)\n",
    "        td = v_t - values\n",
    "        c_loss = td.pow(2)\n",
    "\n",
    "        m = self.distribution(mu, sigma)\n",
    "        log_prob = m.log_prob(a)\n",
    "        entropy = 0.5 + 0.5 * math.log(2 * math.pi) + torch.log(m.scale) # exploration\n",
    "        exp_v = log_prob * td.detach() + 0.005 * entropy\n",
    "        a_loss = -exp_v\n",
    "        total_loss = (a_loss + c_loss).mean()\n",
    "        return total_loss\n",
    "\n",
    "class Worker(mp.Process):\n",
    "    def __init__(self, gnet, opt, global_ep, global_ep_r, res_queue, name):\n",
    "        super(Worker, self).__init__()\n",
    "        self.name = 'w%i' % name\n",
    "        self.g_ep, self.g_ep_r, self.res_queue = global_ep, global_ep_r, res_queue\n",
    "        self.gnet, self.opt = gnet, opt\n",
    "        self.lnet = Net(N_S, N_A) # local network\n",
    "        self.env = gym.make('Pendulum-v1').unwrapped\n",
    "\n",
    "    def run(self):\n",
    "        total_step = 1\n",
    "        while self.g_ep.value < MAX_EP:\n",
    "            s = self.env.reset()\n",
    "            buffer_s, buffer_a, buffer_r = [], [], []\n",
    "            ep_r = 0.\n",
    "            for t in range(MAX_EP_STEP):\n",
    "                if self.name == 'w0':\n",
    "                    self.env.render()\n",
    "                a = self.lnet.choose_action(v_wrap(s[None, :]))\n",
    "                s_, r, done, _ = self.env.step(a.clip(-2, 2))\n",
    "                if t == MAX_EP_STEP - 1:\n",
    "                    done = True\n",
    "                ep_r += r\n",
    "                buffer_a.append(a)\n",
    "                buffer_s.append(s)\n",
    "                buffer_r.append((r+8.1)/8.1) # normalize\n",
    "\n",
    "                if total_step % UPDATE_GLOBAL_ITER == 0 or done:  # update global and assign to local net\n",
    "                    # sync\n",
    "                    push_and_pull(self.opt, self.lnet, self.gnet, done, s_, buffer_s, buffer_a, buffer_r, GAMMA)\n",
    "                    buffer_s, buffer_a, buffer_r = [], [], []\n",
    "\n",
    "                    if done: # done and print information\n",
    "                        record(self.g_ep, self.g_ep_r, ep_r, self.res_queue, self.name)\n",
    "                        break\n",
    "\n",
    "                s = s_\n",
    "                total_step += 1\n",
    "\n",
    "        self.res_queue.put(None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf81ff-0c31-4eac-892b-60a7fa1ce010",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    gnet = Net(N_S, N_A)        # global network\n",
    "    gnet.share_memory()         # share the global parameters in multiprocessing\n",
    "    opt = SharedAdam(gnet.parameters(), lr=1e-2, betas=(0.95, 0.999))  # global optimizer\n",
    "    global_ep, global_ep_r, res_queue = mp.Value('i', 0), mp.Value('d', 0.), mp.Queue()\n",
    "\n",
    "    # parallel training\n",
    "    workers = [Worker(gnet, opt, global_ep, global_ep_r, res_queue, i) for i in range(mp.cpu_count())]\n",
    "    [w.start() for w in workers]\n",
    "    res = []                    # record episode reward to plot\n",
    "    while True:\n",
    "        r = res_queue.get()\n",
    "        if r is not None:\n",
    "            res.append(r)\n",
    "        else:\n",
    "            break\n",
    "    [w.join() for w in workers]\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(res)\n",
    "    plt.ylabel('Moving average ep reward')\n",
    "    plt.xlabel('Step')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb22271-04d6-48c9-8c57-462c13bfd446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
