{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is MDP\n",
    "\n",
    "At its core, an MDP is a mathematical framework for decision-making in uncertain environments, where outcomes are partly random and partly under the control of an agent. \n",
    "\n",
    "\"A game where at each step, you're in a certain situation, you take an action, something random happens, and then you end up in a new situation with a reward. The goal is to make a sequence of smart choices to earn the most reward over time.\"\n",
    "\n",
    "5 Key ingredients of an MDP:\n",
    "1. State (S) - \"Where you are\"\n",
    "    - Each state represents a situation the agent can be in\n",
    "    - Example: In a maze, a state could be your current location\n",
    "\n",
    "2. Actions (A) - \"What you can do\"\n",
    "    - From any state, the agent can choose from a set of actions\n",
    "    - Example: In the maze, actions could be \"go left\", \"go right\", \"go up\", or \"go down\". \n",
    "\n",
    "3. Transition probabilities (P) - \"What happens when you act\"\n",
    "    - These define the rules of the world: If you take an action in a state, what is the chance you'll end up in a particular next state?\n",
    "    - Example: You press \"go up\" from cell A2, but there's a 10% chance you slip and end up in A1 instead of A3. \n",
    "\n",
    "4. Rewards (R) - \"How good was that move?\"\n",
    "    - After each action, you get a reward - a number indicating how good or bad the result is.\n",
    "    - Example: Reaching the goal gives you +10, stepping into lava gives you -100, and walking on empty tiles gives you 0. \n",
    "\n",
    "5. Discount factor ($\\gamma$) - \"How much do you care about the future?\"\n",
    "    - This is a number between 0 and 1 that tells the agent how far into the future it should care about rewards. \n",
    "    - If $\\gamma$ is close to 1, you care about long-term rewards.\n",
    "    - If $\\gamma$ is close to 0, you only care about immediate rewards. \n",
    "\n",
    "\n",
    "What's the \"Markov\" part?\n",
    "The Markov property means:\n",
    "\"The future only depends on the current state and action, not on the history of how you got there.\"\n",
    "So, the agent doesn't need to remember the full path - just where it is now and what it cana do. \n",
    "\n",
    "What's the Agent Trying to Learn?\n",
    "The agent wants to learn a policy:\n",
    "\"A rule that says: \"If I'm in state X, I should take action Y.\"\n",
    "The ultimate goal is to find the optimal policy - one that gives the highest total expected reward over time. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking the Markov Assumption: When History Matters\n",
    "\n",
    "In many real-world problems, \"where you've been\" does matter. Experience shapes decisions. And just like in life, the assumption of \"memoryless\" (the Markov property) can be too limiting for smart agents.\n",
    "\n",
    "The standard MDP assumes: \n",
    "\"The next state and reward depend only on the current state and action - not on the path taken to get there\"\n",
    "\n",
    "But in many problems:\n",
    " - The state isn't fully observable\n",
    " - There's hidden information, delayed effects, or ambiguous situations\n",
    " - A single observation isn't enough to act optimally - you need context\n",
    "\n",
    "Solutions\n",
    "1. Recurrent Neural Networks (RNNs) / LSTMs\n",
    "\n",
    "    In Deep RL: \n",
    "    * Use an RNN or LSTM as part of the policy or value network\n",
    "    * This allows the agent to summarize past observations into a hidden state - essentially giving it a memory\n",
    "    * Now your policy is a function of history, not just current state\n",
    "        $$ \\pi (a_t | o_{1:t}) \\text{ instead of } \\pi (a_t | o_t) $$\n",
    "\n",
    "2. Experience Replay\n",
    "\n",
    "    While this doesn't help in real-time decision-making, it's about learning from past episodes\n",
    "    * The agent stores a buffer or transitions (s, a, r, s') and reuses them during training \n",
    "    * Like journaling your past mistakes and learning from them in retrospect\n",
    "\n",
    "3. Learning from Trajectories / Behavior Cloning\n",
    "\n",
    "    Train agents not just on current transitions but entire sequences of actions\n",
    "    * For example, imitation learning or inverse reinforcement learning often uses trajectory-level data to infer goals or strategies\n",
    "\n",
    "Beyond Memory: Identity, Strategy,Growth\n",
    "* Meta-RL: where the agent learns how to learn from its past episodes across different tasks\n",
    "* Curriculum learning: building up competence through progressively harder challenges - much like gaining wisdom through life. \n",
    "* Lifelong / continual learning: retaining knowledge across tasks adn avoiding forgetting old skills\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A World-Modeling Agent\n",
    "\n",
    "Humans don't just react. We anticipate, generalize, and simulate the future based on our internal understanding of how the world works. In RL terms, I am aiming for:\n",
    "\" Agents that learn not just policies, but internal models of the environment. These models are then used to reason, simulate, and plan - like we do in our heads.\"\n",
    "\n",
    "_\"Model-Based RL\" - The First Step Toward Understanding_\n",
    "\n",
    "There are two broad types of RL\n",
    "1. Model-Free RL\n",
    "    * Learns policy/value directly from experience\n",
    "    * Doesn't build any understanding of the environment\n",
    "    * \"I don't know why, but when I do this, I get rewarded\"\n",
    "\n",
    "2. Model-Based RL:\n",
    "    * Tries to learn a model of the environment \n",
    "    $$ f(s,a) \\rightarrow s', r $$\n",
    "    * The uses this learned model to simulate outcomes, plan ahead, or optimize actions\n",
    "    * Much more data-efficient, interpretable, and aligned with how humans operate\n",
    "    * Challenge: learned models are often imperfect, and compounding errors can sabotage planning\n",
    "\n",
    "\n",
    "_The Frontier: Agents that Understand the \"Why\"_\n",
    "\n",
    "We actually want a structured world models or abstract reasoning. \n",
    "* Not just learning to predict, but learning why things happen\n",
    "* Not just optimizing a reward, but understanding the structure and rules of the environment\n",
    "* Not just reacting, but explaining - being able to say\n",
    "    \"If I do X, Y will likely happen because that's how this world tends to behave\"\n",
    "\n",
    "This connects to areas like:\n",
    "1. World Models \n",
    "* The agent learns a latent-space simulator of the environment (like a mental imagination engine)\n",
    "* Then it uses this simulator for planning actions without needing the real environment\n",
    "* Very aligned with the idea: learn how the world works, not just what actions are best\n",
    "\n",
    "2. Causal Reinforcement Learning:\n",
    "* Learn the causal structure of the environment: \"What causes what?\"\n",
    "* Allows better generalization, transfer learning, and counterfactual reasoning\n",
    "* Example: Knowing that turning off the stove causes the fire to go out - rather than just observing correlation\n",
    "* Strong overlap with real-world reasoning\n",
    "\n",
    "3. Self-Reflective and Meta-Cognitive Agents\n",
    "* Agent that don't just act, but also reflect on their experiences\n",
    "* Like humans journaling and extracting lessons over time. \n",
    "* In RL, this is an area that blends meta-learning with world modeling\n",
    "\n",
    "\n",
    "_A Practical Vision We Could Build Toward_:\n",
    "\n",
    "1. Experience: Agent explores an environment (e.g., a factory process, a game, or a simulated chemistry system)\n",
    "2. Model Learning: It builds an internal model of the system's dynamics. \n",
    "3. Abstraction: Over time, it starts clustering patterns, learning rules, and predicting outcomes beyond raw memorization\n",
    "4. Reasoning: Agent begins to simulate potential futures, even ones it hasn't directly experienced\n",
    "5. Intuition: Given a new state, the agent doesn't just act - it can say:\n",
    "    - \"Given what I know about this kind of situation, the best action is likely X\"\n",
    "6. Explanation: Optionally, it could output:\n",
    "    - \"I chose X because last time this condition led to a delayed failure, and this structure often signals that.\"\n",
    "\n",
    "\n",
    "_Research Concepts to Explore_:\n",
    "\n",
    "* Model-based RL: Learning dynamics and using them to plan\n",
    "* World Models: Compress high-dimensional observations into internal simulations\n",
    "* Causal Inference in RL: Learning cause-effect relationships\n",
    "* Structured Learning: Learning with graphs, rules, or symbolic representations\n",
    "* Meta-RL: Learning how to learn across many tasks\n",
    "* Explaniable RL: Agents that can articulate why they chose actions\n",
    "* Theory of Mind (ToM): Agents that model other's beliefs, intentions, and learning processes (multi-agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
