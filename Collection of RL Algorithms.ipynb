{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c30b100a-7ac7-498e-adc7-b24c9980f9f1",
   "metadata": {},
   "source": [
    "### 1. REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4a623c-3910-497f-a360-84321712c8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : 25.65\n",
      "# of episode :40, avg score : 20.45\n",
      "# of episode :60, avg score : 22.05\n",
      "# of episode :80, avg score : 19.55\n",
      "# of episode :100, avg score : 23.2\n",
      "# of episode :120, avg score : 23.35\n",
      "# of episode :140, avg score : 26.05\n",
      "# of episode :160, avg score : 30.65\n",
      "# of episode :180, avg score : 26.95\n",
      "# of episode :200, avg score : 29.3\n",
      "# of episode :220, avg score : 32.7\n",
      "# of episode :240, avg score : 41.45\n",
      "# of episode :260, avg score : 36.8\n",
      "# of episode :280, avg score : 40.1\n",
      "# of episode :300, avg score : 42.0\n",
      "# of episode :320, avg score : 36.9\n",
      "# of episode :340, avg score : 30.95\n",
      "# of episode :360, avg score : 40.25\n",
      "# of episode :380, avg score : 35.7\n",
      "# of episode :400, avg score : 40.05\n",
      "# of episode :420, avg score : 54.0\n",
      "# of episode :440, avg score : 40.2\n",
      "# of episode :460, avg score : 43.65\n",
      "# of episode :480, avg score : 51.9\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "gamma         = 0.98\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.data = [] # stores rewards nd action probabilities for policy gradient updates\n",
    "        \n",
    "        self.fc1 = nn.Linear(4, 128) # 4 input neurons CartPole state size, 128 neurons\n",
    "        self.fc2 = nn.Linear(128, 2) # 128 neurons and 2 output actions: left or right\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate) # optimizer for training the policy network\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # introduces non-linearity\n",
    "        x = F.softmax(self.fc2(x), dim=0) # converts outputs into probability distributions over actions\n",
    "        return x\n",
    "      \n",
    "    def put_data(self, item):\n",
    "        self.data.append(item)\n",
    "        \n",
    "    def train_net(self):\n",
    "        R = 0 # cumulative reward\n",
    "        self.optimizer.zero_grad() # reset gradients\n",
    "        for r, prob in self.data[::-1]:\n",
    "            R = r + gamma * R # discounted reward\n",
    "            loss = -torch.log(prob) * R # maximizes probability of good actions (higher rewards); minimizes p of bad actions\n",
    "            loss.backward()   # computes gradients\n",
    "        self.optimizer.step() # updates policy network\n",
    "        self.data = [] \n",
    "\n",
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    pi = Policy()\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "    \n",
    "    \n",
    "    for n_epi in range(500):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done: # CartPole-v1 forced to terminates at 500 step.\n",
    "            prob = pi(torch.from_numpy(s).float()) # compute action probabilities\n",
    "            m = Categorical(prob) # create a probability distribution\n",
    "            a = m.sample() # sample an action\n",
    "            s_prime, r, done, truncated, info = env.step(a.item()) # take action in env\n",
    "            pi.put_data((r,prob[a])) # store reward and action probability\n",
    "            s = s_prime # move to the next state\n",
    "            score += r # accumulate score\n",
    "            \n",
    "        pi.train_net()\n",
    "        \n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {}\".format(n_epi, score/print_interval))\n",
    "            score = 0.0\n",
    "    env.close()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f8421-662f-46d0-b126-cdcd406c33d3",
   "metadata": {},
   "source": [
    "### 2. Vallina Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea4d6102-001a-42b7-b8ee-cbc18a497efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\26088\\AppData\\Local\\Temp\\ipykernel_15728\\3532658833.py:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  s_batch, a_batch, r_batch, s_prime_batch, done_batch = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : 22.6\n",
      "# of episode :40, avg score : 19.6\n",
      "# of episode :60, avg score : 30.0\n",
      "# of episode :80, avg score : 28.9\n",
      "# of episode :100, avg score : 23.8\n",
      "# of episode :120, avg score : 32.1\n",
      "# of episode :140, avg score : 43.5\n",
      "# of episode :160, avg score : 50.4\n",
      "# of episode :180, avg score : 52.4\n",
      "# of episode :200, avg score : 52.6\n",
      "# of episode :220, avg score : 69.5\n",
      "# of episode :240, avg score : 75.3\n",
      "# of episode :260, avg score : 113.3\n",
      "# of episode :280, avg score : 79.4\n",
      "# of episode :300, avg score : 121.5\n",
      "# of episode :320, avg score : 148.8\n",
      "# of episode :340, avg score : 183.7\n",
      "# of episode :360, avg score : 197.5\n",
      "# of episode :380, avg score : 138.8\n",
      "# of episode :400, avg score : 198.3\n",
      "# of episode :420, avg score : 277.3\n",
      "# of episode :440, avg score : 320.6\n",
      "# of episode :460, avg score : 232.3\n",
      "# of episode :480, avg score : 273.6\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "gamma         = 0.98\n",
    "n_rollout     = 10 # number of steps to collect before updating the model\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        self.fc1 = nn.Linear(4,256) # shared feature extraction layer \n",
    "        self.fc_pi = nn.Linear(256,2) # actor network, outputs action probabilities\n",
    "        self.fc_v = nn.Linear(256,1) # critic network, outputs single state-value (V(s))\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = F.relu(self.fc1(x)) # pass through shared network\n",
    "        x = self.fc_pi(x) # get action logits\n",
    "        prob = F.softmax(x, dim=softmax_dim) # convert to probabilities\n",
    "        return prob\n",
    "    \n",
    "    def v(self, x): # critic network (value function)\n",
    "        x = F.relu(self.fc1(x)) # pass through shared network \n",
    "        v = self.fc_v(x) # output value estimate\n",
    "        return v\n",
    "    \n",
    "    def put_data(self, transition): \n",
    "        self.data.append(transition) # store (state, action, reward, next state, done)\n",
    "        \n",
    "    def make_batch(self): # converts stored data into mini-batches for training\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            s,a,r,s_prime,done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r/100.0]) # normalize reward\n",
    "            s_prime_lst.append(s_prime) \n",
    "            done_mask = 0.0 if done else 1.0 # Done flag; helps bootstrapping\n",
    "            done_lst.append([done_mask])\n",
    "\n",
    "        # convert list to PyTorch tensors\n",
    "        s_batch, a_batch, r_batch, s_prime_batch, done_batch = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "                                                               torch.tensor(r_lst, dtype=torch.float), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                                                               torch.tensor(done_lst, dtype=torch.float)\n",
    "        self.data = []\n",
    "        return s_batch, a_batch, r_batch, s_prime_batch, done_batch\n",
    "  \n",
    "    def train_net(self):\n",
    "        s, a, r, s_prime, done = self.make_batch() \n",
    "\n",
    "        # compute TD target : R + Î³ * V(s')\n",
    "        td_target = r + gamma * self.v(s_prime) * done  # bootstraps future rewards\n",
    "        delta = td_target - self.v(s) # compute TD error (target-value)\n",
    "\n",
    "        # compute policy loss\n",
    "        pi = self.pi(s, softmax_dim=1) # Compute action probabilities\n",
    "        pi_a = pi.gather(1,a) # Select probabilities corresponding to actions taken\n",
    "        loss = -torch.log(pi_a) * delta.detach() # updates policy based on TD error\n",
    "        loss += F.smooth_l1_loss(self.v(s), td_target.detach()) # critic loss: trains value function\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()         \n",
    "      \n",
    "def main():  \n",
    "    env = gym.make('CartPole-v1')\n",
    "    model = ActorCritic()    \n",
    "    print_interval = 20\n",
    "    score = 0.0\n",
    "\n",
    "    for n_epi in range(500):\n",
    "        done = False\n",
    "        s, _ = env.reset()\n",
    "        while not done:\n",
    "            for t in range(n_rollout): # collect experiences\n",
    "                prob = model.pi(torch.from_numpy(s).float()) # get action probabilities\n",
    "                m = Categorical(prob) \n",
    "                a = m.sample().item() # sample an action\n",
    "\n",
    "                # take action\n",
    "                s_prime, r, done, truncated, info = env.step(a)\n",
    "\n",
    "                # store transition\n",
    "                model.put_data((s,a,r,s_prime,done))\n",
    "                \n",
    "                s = s_prime\n",
    "                score += r\n",
    "                \n",
    "                if done:\n",
    "                    break                     \n",
    "            \n",
    "            model.train_net() # train after \"n_rollout\" steps\n",
    "            \n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
    "            score = 0.0\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33335878-9a4c-403e-9bac-1030d8799b90",
   "metadata": {},
   "source": [
    "### 3. DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba6cc42-2e39-43ce-bcf5-e0ff37df1ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode :20, score : 14.8, n_buffer : 296, eps : 7.9%\n",
      "n_episode :40, score : 12.1, n_buffer : 537, eps : 7.8%\n",
      "n_episode :60, score : 13.4, n_buffer : 806, eps : 7.7%\n",
      "n_episode :80, score : 13.1, n_buffer : 1068, eps : 7.6%\n",
      "n_episode :100, score : 17.1, n_buffer : 1409, eps : 7.5%\n",
      "n_episode :120, score : 16.1, n_buffer : 1731, eps : 7.4%\n",
      "n_episode :140, score : 13.2, n_buffer : 1995, eps : 7.3%\n",
      "n_episode :160, score : 14.3, n_buffer : 2282, eps : 7.2%\n",
      "n_episode :180, score : 26.2, n_buffer : 2807, eps : 7.1%\n",
      "n_episode :200, score : 21.3, n_buffer : 3233, eps : 7.0%\n",
      "n_episode :220, score : 24.1, n_buffer : 3715, eps : 6.9%\n",
      "n_episode :240, score : 14.6, n_buffer : 4006, eps : 6.8%\n",
      "n_episode :260, score : 15.0, n_buffer : 4306, eps : 6.7%\n",
      "n_episode :280, score : 20.2, n_buffer : 4710, eps : 6.6%\n",
      "n_episode :300, score : 15.4, n_buffer : 5000, eps : 6.5%\n",
      "n_episode :320, score : 28.4, n_buffer : 5000, eps : 6.4%\n",
      "n_episode :340, score : 23.8, n_buffer : 5000, eps : 6.3%\n",
      "n_episode :360, score : 124.2, n_buffer : 5000, eps : 6.2%\n",
      "n_episode :380, score : 114.2, n_buffer : 5000, eps : 6.1%\n",
      "n_episode :400, score : 331.5, n_buffer : 5000, eps : 6.0%\n",
      "n_episode :420, score : 316.6, n_buffer : 5000, eps : 5.9%\n",
      "n_episode :440, score : 318.8, n_buffer : 5000, eps : 5.8%\n",
      "n_episode :460, score : 292.6, n_buffer : 5000, eps : 5.7%\n",
      "n_episode :480, score : 434.0, n_buffer : 5000, eps : 5.6%\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma         = 0.98\n",
    "buffer_limit  = 5000 # maximum size of the replay buffer\n",
    "batch_size    = 32 # number of samples per training iteration\n",
    "\n",
    "# stores past experiences (s,a,r,s'done_mask) in a fixed-sized buffer\n",
    "# breaks correlation between consecutive experiences\n",
    "# allows mini-batch training, improving sample efficiency\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "    # put a new experience to the buffer\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    # randomly samples n transitions from the buffer\n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "               torch.tensor(done_mask_lst)\n",
    "    # returns the current buffer size\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Deep Q-Network DQN\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    # uses epsilon greedy Policy\n",
    "    # with probability epsilon, selects a random action (exploration)\n",
    "    # otherwise, selects the best action using argmax(Q(s,a)) (exploitation)\n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs)\n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0,1)\n",
    "        else : \n",
    "            return out.argmax().item()\n",
    "            \n",
    "def train(q, q_target, memory, optimizer):\n",
    "    for i in range(10):\n",
    "        s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "\n",
    "        q_out = q(s) # compute Q-values for all actions in state s\n",
    "        q_a = q_out.gather(1,a) # select Q-values corresponding to actions taken \n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1) # computes the target Q-value (max Q(s', a'))\n",
    "        target = r + gamma * max_q_prime * done_mask #Bellman equation: Computes the expected Q-value.\n",
    "        loss = F.smooth_l1_loss(q_a, target) # Uses Huber loss to stabilize training.\n",
    "         \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    q = Qnet() # online Q-network\n",
    "    q_target = Qnet() # target Q-network\n",
    "    q_target.load_state_dict(q.state_dict()) # initilize target network\n",
    "    memory = ReplayBuffer()\n",
    "\n",
    "    print_interval = 20\n",
    "    score = 0.0  \n",
    "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "\n",
    "    for n_epi in range(500):\n",
    "        epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) #Linear annealing from 8% to 1%\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            a = q.sample_action(torch.from_numpy(s).float(), epsilon) # randomly selects action  \n",
    "            s_prime, r, done, truncated, info = env.step(a) # take action\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            memory.put((s,a,r/100.0,s_prime, done_mask)) # store info\n",
    "            s = s_prime\n",
    "\n",
    "            score += r\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        if memory.size()>2000:\n",
    "            train(q, q_target, memory, optimizer)\n",
    "\n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            q_target.load_state_dict(q.state_dict())\n",
    "            print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
    "                                                            n_epi, score/print_interval, memory.size(), epsilon*100))\n",
    "            score = 0.0\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9229d6c6-7af5-4127-bc7b-6926376ce16c",
   "metadata": {},
   "source": [
    "### 4. PPO (Proximal Policy Gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08f77585-06e8-47c9-aab8-a2332a4a125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : 44.7\n",
      "# of episode :40, avg score : 53.0\n",
      "# of episode :60, avg score : 63.8\n",
      "# of episode :80, avg score : 96.5\n",
      "# of episode :100, avg score : 55.7\n",
      "# of episode :120, avg score : 75.8\n",
      "# of episode :140, avg score : 191.4\n",
      "# of episode :160, avg score : 186.7\n",
      "# of episode :180, avg score : 288.0\n",
      "# of episode :200, avg score : 232.9\n",
      "# of episode :220, avg score : 129.1\n",
      "# of episode :240, avg score : 109.8\n",
      "# of episode :260, avg score : 349.1\n",
      "# of episode :280, avg score : 233.1\n",
      "# of episode :300, avg score : 297.1\n",
      "# of episode :320, avg score : 366.1\n",
      "# of episode :340, avg score : 814.8\n",
      "# of episode :360, avg score : 389.9\n",
      "# of episode :380, avg score : 105.7\n",
      "# of episode :400, avg score : 1718.3\n",
      "# of episode :420, avg score : 58.4\n",
      "# of episode :440, avg score : 60.4\n",
      "# of episode :460, avg score : 83.4\n",
      "# of episode :480, avg score : 86.2\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma         = 0.98\n",
    "lmbda         = 0.95 # GAE (generalized advantage estimation) decay factor\n",
    "eps_clip      = 0.1 # clipping parameter for PPO (prevents large updates)\n",
    "K_epoch       = 3 # number of PPO updates per batch\n",
    "T_horizon     = 10 # number of steps collected before training\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        self.fc1   = nn.Linear(4,256) \n",
    "        self.fc_pi = nn.Linear(256,2) # actor network\n",
    "        self.fc_v  = nn.Linear(256,1) # critic network\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate) \n",
    "\n",
    "    # policy (actor)\n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=softmax_dim)\n",
    "        return prob\n",
    "\n",
    "    # value (critic)\n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "\n",
    "    # stores tuples (state, action, reward, next_state, action_prob, done_flag)\n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            s, a, r, s_prime, prob_a, done = transition\n",
    "            # convert tuple into PyTorch Tensors\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            prob_a_lst.append([prob_a])\n",
    "            done_mask = 0 if done else 1\n",
    "            done_lst.append([done_mask])\n",
    "            \n",
    "        s,a,r,s_prime,done_mask, prob_a = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "                                          torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                                          torch.tensor(done_lst, dtype=torch.float), torch.tensor(prob_a_lst)\n",
    "        self.data = []\n",
    "        return s, a, r, s_prime, done_mask, prob_a\n",
    "        \n",
    "    def train_net(self):\n",
    "        s, a, r, s_prime, done_mask, prob_a = self.make_batch()\n",
    "\n",
    "        for i in range(K_epoch): # PPO update steps\n",
    "            td_target = r + gamma * self.v(s_prime) * done_mask # estimates expected return\n",
    "            delta = td_target - self.v(s) # TD error\n",
    "            delta = delta.detach().numpy()\n",
    "\n",
    "            # Generalized Advantage Estimation\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]: # reverse iterate through batch\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0] # computes advantage function using GAE\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "\n",
    "            # computing the PPO loss\n",
    "            pi = self.pi(s, softmax_dim=1)\n",
    "            pi_a = pi.gather(1,a)\n",
    "            # pi_a / prob_a : measures how much the new policy differs from the old policy\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "            \n",
    "            surr1 = ratio * advantage\n",
    "            # clipping prevents large policy updates (stablizes learning)\n",
    "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "            # implements PPO clipped surrogate objective\n",
    "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target.detach())\n",
    "\n",
    "            # computates gradients and updates parameters\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    model = PPO()\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "\n",
    "    for n_epi in range(500):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            for t in range(T_horizon):\n",
    "                prob = model.pi(torch.from_numpy(s).float())\n",
    "                m = Categorical(prob) # selects actions using a stochastic policy\n",
    "                a = m.sample().item()\n",
    "                s_prime, r, done, truncated, info = env.step(a)\n",
    "\n",
    "                model.put_data((s, a, r/100.0, s_prime, prob[a].item(), done))\n",
    "                s = s_prime\n",
    "\n",
    "                score += r\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            model.train_net()\n",
    "\n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
    "            score = 0.0\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a482d6-3c31-4f1b-92df-dfb069dfe127",
   "metadata": {},
   "source": [
    "### 5. DDPG (Deep Deterministic Policy Gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c4db866-680b-4709-be87-5af85047bab6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PendulumEnv.__init__() got an unexpected keyword argument 'autoreset' was raised from the environment creator for Pendulum-v1 with kwargs ({'autoreset': True})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\gymnasium\\envs\\registration.py:740\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 740\u001b[0m     env \u001b[38;5;241m=\u001b[39m env_creator(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menv_spec_kwargs)\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: PendulumEnv.__init__() got an unexpected keyword argument 'autoreset'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 161\u001b[0m\n\u001b[0;32m    158\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 161\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[4], line 117\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# a continuous action env\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPendulum-v1\u001b[39m\u001b[38;5;124m'\u001b[39m, max_episode_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, autoreset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    118\u001b[0m     memory \u001b[38;5;241m=\u001b[39m ReplayBuffer()\n\u001b[0;32m    120\u001b[0m     q, q_target \u001b[38;5;241m=\u001b[39m QNet(), QNet()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\gymnasium\\envs\\registration.py:752\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\n\u001b[0;32m    747\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed render_mode=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m although \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_spec\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt implement human-rendering natively. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    748\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGym tried to apply the HumanRendering wrapper but it looks like your environment is using the old \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    749\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrendering API, which is not supported by the HumanRendering wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    750\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 752\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\n\u001b[0;32m    753\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was raised from the environment creator for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_spec\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with kwargs (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_spec_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    754\u001b[0m         )\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env, gym\u001b[38;5;241m.\u001b[39mEnv):\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    758\u001b[0m         \u001b[38;5;28mstr\u001b[39m(env\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m__base__) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgym.core.Env\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    759\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(env\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m__base__) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgym.core.Wrapper\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    760\u001b[0m     ):\n",
      "\u001b[1;31mTypeError\u001b[0m: PendulumEnv.__init__() got an unexpected keyword argument 'autoreset' was raised from the environment creator for Pendulum-v1 with kwargs ({'autoreset': True})"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Hyperparameters\n",
    "lr_mu        = 0.0005\n",
    "lr_q         = 0.001\n",
    "gamma        = 0.99\n",
    "batch_size   = 32\n",
    "buffer_limit = 5000\n",
    "tau          = 0.005 # for target network soft update\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask = 0.0 if done else 1.0 \n",
    "            done_mask_lst.append([done_mask])\n",
    "        \n",
    "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst, dtype=torch.float), \\\n",
    "                torch.tensor(r_lst, dtype=torch.float), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                torch.tensor(done_mask_lst, dtype=torch.float)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# actor network\n",
    "class MuNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MuNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc_mu = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = torch.tanh(self.fc_mu(x))*2 # Multipled by 2 because the action space of the Pendulum-v0 is [-2,2]\n",
    "        return mu\n",
    "\n",
    "# critic network (Q-network)\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QNet, self).__init__()\n",
    "        self.fc_s = nn.Linear(3, 64)\n",
    "        self.fc_a = nn.Linear(1,64)\n",
    "        self.fc_q = nn.Linear(128, 32)\n",
    "        self.fc_out = nn.Linear(32,1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        h1 = F.relu(self.fc_s(x))\n",
    "        h2 = F.relu(self.fc_a(a))\n",
    "        cat = torch.cat([h1,h2], dim=1)\n",
    "        q = F.relu(self.fc_q(cat))\n",
    "        q = self.fc_out(q)\n",
    "        return q\n",
    "\n",
    "# exploration strategy: Ornstein-Uhlenbeck Noise\n",
    "# Handles exploration in continuous action spaces.\n",
    "# Ornstein-Uhlenbeck Noise introduces correlated noise, useful for smooth actions.\n",
    "\n",
    "class OrnsteinUhlenbeckNoise:\n",
    "    def __init__(self, mu):\n",
    "        self.theta, self.dt, self.sigma = 0.1, 0.01, 0.1\n",
    "        self.mu = mu\n",
    "        self.x_prev = np.zeros_like(self.mu)\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "# updating networks\n",
    "def train(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer):\n",
    "    s,a,r,s_prime,done_mask  = memory.sample(batch_size)\n",
    "\n",
    "    # critic loss\n",
    "    target = r + gamma * q_target(s_prime, mu_target(s_prime)) * done_mask\n",
    "    q_loss = F.smooth_l1_loss(q(s,a), target.detach())\n",
    "    q_optimizer.zero_grad()\n",
    "    q_loss.backward()\n",
    "    q_optimizer.step()\n",
    "\n",
    "    # actor loss\n",
    "    mu_loss = -q(s,mu(s)).mean() # maximize expected Q-value\n",
    "    mu_optimizer.zero_grad()\n",
    "    mu_loss.backward()\n",
    "    mu_optimizer.step()\n",
    "\n",
    "# target network soft update\n",
    "# prevents dramatic Q-value changes\n",
    "def soft_update(net, net_target):\n",
    "    for param_target, param in zip(net_target.parameters(), net.parameters()):\n",
    "        param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)\n",
    "    \n",
    "def main():\n",
    "    # a continuous action env\n",
    "    env = gym.make('Pendulum-v1', max_episode_steps=50, autoreset=True)\n",
    "    memory = ReplayBuffer()\n",
    "\n",
    "    q, q_target = QNet(), QNet()\n",
    "    q_target.load_state_dict(q.state_dict())\n",
    "    mu, mu_target = MuNet(), MuNet()\n",
    "    mu_target.load_state_dict(mu.state_dict())\n",
    "\n",
    "    score = 0.0\n",
    "    print_interval = 10\n",
    "\n",
    "    mu_optimizer = optim.Adam(mu.parameters(), lr=lr_mu)\n",
    "    q_optimizer  = optim.Adam(q.parameters(), lr=lr_q)\n",
    "    ou_noise = OrnsteinUhlenbeckNoise(mu=np.zeros(1))\n",
    "\n",
    "    for n_epi in range(500):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        count = 0\n",
    "        while count < 200 and not done:\n",
    "            a = mu(torch.from_numpy(s).float())  # take an action using mu, adds noise for exploration\n",
    "            a = a.item() + ou_noise()[0] # add exploration noise \n",
    "            s_prime, r, done, truncated, info = env.step([a]) # replay buffer\n",
    "            memory.put((s,a,r/100.0,s_prime,done))\n",
    "            score +=r\n",
    "            s = s_prime\n",
    "            count += 1\n",
    "        # training and soft update\n",
    "        # trains on a batch of samples from the buffer\n",
    "        # performs soft updates on both actor and critic target networks\n",
    "        if memory.size()>1000:\n",
    "            for i in range(10):\n",
    "                train(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer)\n",
    "                soft_update(mu, mu_target)\n",
    "                soft_update(q,  q_target)\n",
    "        \n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
    "            score = 0.0\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "# Quick summary\n",
    "# replay buffer -> stores past experiences for training\n",
    "# actor network (mu) -> maps states to continuous actions\n",
    "# critic network (Q) -> evaluates state-action pairs\n",
    "# Ornstein-Uhlenbeck Noise -> smooths exploration in continuous space \n",
    "# target networks (mu_target, Q_target) -> improves stability, prevents divergence\n",
    "# soft target updates -> gradually updates target networks (\\tau-weighted average)\n",
    "# policy gradient update -> maximizes expected Q-values\n",
    "# critic update -> uses bellman equation to estimate q-values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201b9fd6-1199-4c17-817b-e02b7416137b",
   "metadata": {},
   "source": [
    "### 6. A3C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fdc46e6-0a34-4721-9036-9a2bfbd2d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.multiprocessing as mp\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "n_train_processes = 3\n",
    "learning_rate = 0.0002\n",
    "update_interval = 5\n",
    "gamma = 0.98\n",
    "max_train_ep = 100\n",
    "max_test_ep = 150\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 256)\n",
    "        self.fc_pi = nn.Linear(256, 2)\n",
    "        self.fc_v = nn.Linear(256, 1)\n",
    "\n",
    "    def pi(self, x, softmax_dim=0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=softmax_dim)\n",
    "        return prob\n",
    "\n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "\n",
    "# training multiple processes in parallel\n",
    "def train(global_model, rank):\n",
    "    local_model = ActorCritic() # copies global model into its local model\n",
    "    local_model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "    optimizer = optim.Adam(global_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    env = gym.make('CartPole-v1')\n",
    "\n",
    "    # collecting trajectories (experience replay)\n",
    "    for n_epi in range(max_train_ep):\n",
    "        done = False\n",
    "        s = env.reset()\n",
    "        while not done:\n",
    "            s_lst, a_lst, r_lst = [], [], []\n",
    "            for t in range(update_interval):\n",
    "                prob = local_model.pi(torch.from_numpy(s).float())\n",
    "                m = Categorical(prob) # selects an action stochastically\n",
    "                a = m.sample().item()\n",
    "                s_prime, r, done, info = env.step(a)\n",
    "\n",
    "                s_lst.append(s)\n",
    "                a_lst.append([a])\n",
    "                r_lst.append(r/100.0)\n",
    "\n",
    "                s = s_prime\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # computes the advantage function\n",
    "            s_final = torch.tensor(s_prime, dtype=torch.float)\n",
    "            R = 0.0 if done else local_model.v(s_final).item()\n",
    "            td_target_lst = []\n",
    "            for reward in r_lst[::-1]:\n",
    "                R = gamma * R + reward\n",
    "                td_target_lst.append([R])\n",
    "            td_target_lst.reverse()\n",
    "\n",
    "            # advantage function: \n",
    "            # measures how much better an action is compared to the baseline (state value estimate)\n",
    "            s_batch, a_batch, td_target = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "                torch.tensor(td_target_lst)\n",
    "            advantage = td_target - local_model.v(s_batch) # TD target - V(s)\n",
    "\n",
    "            # policy gradient loss\n",
    "            pi = local_model.pi(s_batch, softmax_dim=1)\n",
    "            pi_a = pi.gather(1, a_batch)\n",
    "            loss = -torch.log(pi_a) * advantage.detach() + \\\n",
    "                F.smooth_l1_loss(local_model.v(s_batch), td_target.detach()) # stablize training\n",
    "\n",
    "            # updating global model\n",
    "            optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            for global_param, local_param in zip(global_model.parameters(), local_model.parameters()):\n",
    "                global_param._grad = local_param.grad\n",
    "            optimizer.step()\n",
    "            local_model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "    env.close()\n",
    "    print(\"Training process {} reached maximum episode.\".format(rank))\n",
    "\n",
    "\n",
    "def test(global_model):\n",
    "    env = gym.make('CartPole-v1')\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "\n",
    "    for n_epi in range(max_test_ep):\n",
    "        done = False\n",
    "        s = env.reset()\n",
    "        while not done:\n",
    "            prob = global_model.pi(torch.from_numpy(s).float())\n",
    "            a = Categorical(prob).sample().item()\n",
    "            s_prime, r, done, info = env.step(a)\n",
    "            s = s_prime\n",
    "            score += r\n",
    "\n",
    "        if n_epi % print_interval == 0 and n_epi != 0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}\".format(\n",
    "                n_epi, score/print_interval))\n",
    "            score = 0.0\n",
    "            time.sleep(1)\n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # the global model is shared across multiple training processes \n",
    "    # each process runs independently, collecting experience and updating the global model\n",
    "    global_model = ActorCritic()\n",
    "    global_model.share_memory()\n",
    "\n",
    "    processes = []\n",
    "    # spawns multiple processes, n_train_processes for training, 1 for testing\n",
    "    for rank in range(n_train_processes + 1):  # + 1 for test process\n",
    "        if rank == 0:\n",
    "            p = mp.Process(target=test, args=(global_model,))\n",
    "        else:\n",
    "            p = mp.Process(target=train, args=(global_model, rank,))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    # each process operates asynchronously and updates the global model\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215efc06-c98a-4073-9d5f-42f36d3e35b5",
   "metadata": {},
   "source": [
    "### 7. ACER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac5b2c43-8f33-4652-9093-17c8e6ae8a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : 23.3, buffer size : 56\n",
      "# of episode :40, avg score : 27.2, buffer size : 120\n",
      "# of episode :60, avg score : 34.0, buffer size : 195\n",
      "# of episode :80, avg score : 33.0, buffer size : 270\n",
      "# of episode :100, avg score : 42.4, buffer size : 363\n",
      "# of episode :120, avg score : 60.8, buffer size : 494\n",
      "# of episode :140, avg score : 76.0, buffer size : 656\n",
      "# of episode :160, avg score : 129.3, buffer size : 925\n",
      "# of episode :180, avg score : 184.9, buffer size : 1304\n",
      "# of episode :200, avg score : 270.9, buffer size : 1854\n",
      "# of episode :220, avg score : 263.9, buffer size : 2389\n",
      "# of episode :240, avg score : 294.1, buffer size : 2987\n",
      "# of episode :260, avg score : 280.1, buffer size : 3556\n",
      "# of episode :280, avg score : 360.4, buffer size : 4286\n",
      "# of episode :300, avg score : 292.3, buffer size : 4878\n",
      "# of episode :320, avg score : 305.2, buffer size : 5497\n",
      "# of episode :340, avg score : 436.3, buffer size : 6000\n",
      "# of episode :360, avg score : 404.2, buffer size : 6000\n",
      "# of episode :380, avg score : 356.1, buffer size : 6000\n",
      "# of episode :400, avg score : 1913.3, buffer size : 6000\n",
      "# of episode :420, avg score : 258.5, buffer size : 6000\n",
      "# of episode :440, avg score : 239.9, buffer size : 6000\n",
      "# of episode :460, avg score : 6661.4, buffer size : 6000\n",
      "# of episode :480, avg score : 9.6, buffer size : 6000\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Characteristics\n",
    "# combines value-based learning and policy optimization with off-policy correction. \n",
    "# It is designed for discrete action spaces and operates in a single-threaded env without trust-region updates\n",
    "\n",
    "# 1. Discrete action space, single thread version.\n",
    "# 2. Does not support trust-region updates.\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "gamma         = 0.98\n",
    "buffer_limit  = 6000  \n",
    "rollout_len   = 10   \n",
    "batch_size    = 4     # Indicates 4 sequences per mini-batch (4*rollout_len = 40 samples total)\n",
    "c             = 1.0   # For truncating importance sampling ratio\n",
    "\n",
    "\n",
    "# stores trajectories rather than single transitions\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, seq_data):\n",
    "        self.buffer.append(seq_data)\n",
    "    \n",
    "    def sample(self, on_policy=False):\n",
    "        if on_policy: # use the most recent trajectory\n",
    "            mini_batch = [self.buffer[-1]]\n",
    "        else: # samples randomly from the replay buffer\n",
    "            mini_batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_lst, a_lst, r_lst, prob_lst, done_lst, is_first_lst = [], [], [], [], [], []\n",
    "        for seq in mini_batch:\n",
    "            is_first = True  # Flag for indicating whether the transition is the first item from a sequence\n",
    "            for transition in seq:\n",
    "                s, a, r, prob, done = transition\n",
    "\n",
    "                s_lst.append(s)\n",
    "                a_lst.append([a])\n",
    "                r_lst.append(r)\n",
    "                prob_lst.append(prob)\n",
    "                done_mask = 0.0 if done else 1.0\n",
    "                done_lst.append(done_mask)\n",
    "                is_first_lst.append(is_first)\n",
    "                is_first = False\n",
    "\n",
    "        s,a,r,prob,done_mask,is_first = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "                                        r_lst, torch.tensor(prob_lst, dtype=torch.float), done_lst, \\\n",
    "                                        is_first_lst\n",
    "        return s,a,r,prob,done_mask,is_first\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "      \n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(4,256)\n",
    "        self.fc_pi = nn.Linear(256,2)\n",
    "        self.fc_q = nn.Linear(256,2)\n",
    "        \n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        pi = F.softmax(x, dim=softmax_dim) # probabilities of actions\n",
    "        return pi\n",
    "    \n",
    "    def q(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        q = self.fc_q(x) # q-values for each action\n",
    "        return q\n",
    "      \n",
    "def train(model, optimizer, memory, on_policy=False):\n",
    "    s,a,r,prob,done_mask,is_first = memory.sample(on_policy)\n",
    "    \n",
    "    q = model.q(s)\n",
    "    q_a = q.gather(1,a)\n",
    "    pi = model.pi(s, softmax_dim = 1)\n",
    "    pi_a = pi.gather(1,a) # computes current policy probabilities for chosen actions\n",
    "    v = (q * pi).sum(1).unsqueeze(1).detach() # approximates V(s)\n",
    "    \n",
    "    rho = pi.detach()/prob # measure how much the new policy differs from behavior policy\n",
    "    rho_a = rho.gather(1,a)\n",
    "    rho_bar = rho_a.clamp(max=c)\n",
    "    # clipped importance sampling: limits extreme updates for stability\n",
    "    correction_coeff = (1-c/rho).clamp(min=0) \n",
    "\n",
    "    q_ret = v[-1] * done_mask[-1]\n",
    "    q_ret_lst = []\n",
    "    for i in reversed(range(len(r))):\n",
    "        q_ret = r[i] + gamma * q_ret\n",
    "        q_ret_lst.append(q_ret.item())\n",
    "        q_ret = rho_bar[i] * (q_ret - q_a[i]) + v[i] # retrace (lambda) equation\n",
    "        \n",
    "        if is_first[i] and i!=0:\n",
    "            q_ret = v[i-1] * done_mask[i-1] # When a new sequence begins, q_ret is initialized  \n",
    "\n",
    "    # loss function\n",
    "    q_ret_lst.reverse()\n",
    "    q_ret = torch.tensor(q_ret_lst, dtype=torch.float).unsqueeze(1)\n",
    "    # converts retraced q-value into a tensor for loss computation\n",
    "    loss1 = -rho_bar * torch.log(pi_a) * (q_ret - v) \n",
    "    loss2 = -correction_coeff * pi.detach() * torch.log(pi) * (q.detach()-v) # bias correction term\n",
    "    loss = loss1 + loss2.sum(1) + F.smooth_l1_loss(q_a, q_ret)\n",
    "\n",
    "    # loss breakdown\n",
    "    # policy loss loss 1 ; uses truncated importance sampling\n",
    "    # bias correction term loss 2: adjusts for off-policy bias \n",
    "    # critic loss : ensures q-value match retrace estimates\n",
    "    \n",
    "    # update model\n",
    "    optimizer.zero_grad()\n",
    "    loss.mean().backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    memory = ReplayBuffer()\n",
    "    model = ActorCritic()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    score = 0.0\n",
    "    print_interval = 20    \n",
    "\n",
    "    for n_epi in range(500):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            seq_data = []\n",
    "            for t in range(rollout_len): \n",
    "                prob = model.pi(torch.from_numpy(s).float())\n",
    "                a = Categorical(prob).sample().item()\n",
    "                s_prime, r, done, truncated, info = env.step(a)\n",
    "                seq_data.append((s, a, r/100.0, prob.detach().numpy(), done))\n",
    "\n",
    "                score +=r\n",
    "                s = s_prime\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            memory.put(seq_data)\n",
    "            if memory.size()>100:\n",
    "                train(model, optimizer, memory, on_policy=True)\n",
    "                train(model, optimizer, memory)\n",
    "        \n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}, buffer size : {}\".format(n_epi, score/print_interval, memory.size()))\n",
    "            score = 0.0\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1352400f-631a-4978-bb44-12a48456e11f",
   "metadata": {},
   "source": [
    "### 8. A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7619d47a-f9e1-4809-a637-c66a84ca5f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "n_train_processes = 3\n",
    "learning_rate = 0.0002\n",
    "update_interval = 5\n",
    "gamma = 0.98\n",
    "max_train_steps = 500\n",
    "PRINT_INTERVAL = update_interval * 10\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 256)\n",
    "        self.fc_pi = nn.Linear(256, 2)\n",
    "        self.fc_v = nn.Linear(256, 1)\n",
    "\n",
    "    def pi(self, x, softmax_dim=1):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=softmax_dim)\n",
    "        return prob\n",
    "\n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "\n",
    "# parallel environment handling\n",
    "# to speed up training, use multiple environments running in parallel\n",
    "def worker(worker_id, master_end, worker_end):\n",
    "    master_end.close()  # close master-end in worker processes\n",
    "    env = gym.make('CartPole-v1')\n",
    "    env.seed(worker_id)\n",
    "\n",
    "    while True:\n",
    "        cmd, data = worker_end.recv()\n",
    "        if cmd == 'step':\n",
    "            ob, reward, done, info = env.step(data)\n",
    "            if done:\n",
    "                ob = env.reset()\n",
    "            worker_end.send((ob, reward, done, info))\n",
    "        elif cmd == 'reset':\n",
    "            ob = env.reset()\n",
    "            worker_end.send(ob)\n",
    "        elif cmd == 'reset_task':\n",
    "            ob = env.reset_task()\n",
    "            worker_end.send(ob)\n",
    "        elif cmd == 'close':\n",
    "            worker_end.close()\n",
    "            break\n",
    "        elif cmd == 'get_spaces':\n",
    "            worker_end.send((env.observation_space, env.action_space))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "# master process managing multiple workers\n",
    "class ParallelEnv:\n",
    "    def __init__(self, n_train_processes):\n",
    "        self.nenvs = n_train_processes\n",
    "        self.waiting = False\n",
    "        self.closed = False\n",
    "        self.workers = list()\n",
    "        # use pipe() communication to send/receive data\n",
    "        master_ends, worker_ends = zip(*[mp.Pipe() for _ in range(self.nenvs)])\n",
    "        self.master_ends, self.worker_ends = master_ends, worker_ends\n",
    "\n",
    "        for worker_id, (master_end, worker_end) in enumerate(zip(master_ends, worker_ends)):\n",
    "            p = mp.Process(target=worker,\n",
    "                           args=(worker_id, master_end, worker_end))\n",
    "            p.daemon = True\n",
    "            p.start()\n",
    "            self.workers.append(p)\n",
    "\n",
    "        # Forbid master to use the worker end for messaging\n",
    "        for worker_end in worker_ends:\n",
    "            worker_end.close()\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        for master_end, action in zip(self.master_ends, actions):\n",
    "            master_end.send(('step', action))\n",
    "        self.waiting = True\n",
    "\n",
    "    def step_wait(self):\n",
    "        results = [master_end.recv() for master_end in self.master_ends]\n",
    "        self.waiting = False\n",
    "        obs, rews, dones, infos = zip(*results)\n",
    "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
    "\n",
    "    def reset(self):\n",
    "        for master_end in self.master_ends:\n",
    "            master_end.send(('reset', None))\n",
    "        return np.stack([master_end.recv() for master_end in self.master_ends])\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.step_async(actions)\n",
    "        return self.step_wait()\n",
    "\n",
    "    def close(self):  # For clean up resources\n",
    "        if self.closed:\n",
    "            return\n",
    "        if self.waiting:\n",
    "            [master_end.recv() for master_end in self.master_ends]\n",
    "        for master_end in self.master_ends:\n",
    "            master_end.send(('close', None))\n",
    "        for worker in self.workers:\n",
    "            worker.join()\n",
    "            self.closed = True\n",
    "\n",
    "def test(step_idx, model):\n",
    "    env = gym.make('CartPole-v1')\n",
    "    score = 0.0\n",
    "    done = False\n",
    "    num_test = 10\n",
    "\n",
    "    for _ in range(num_test):\n",
    "        s = env.reset()\n",
    "        while not done:\n",
    "            prob = model.pi(torch.from_numpy(s).float(), softmax_dim=0)\n",
    "            a = Categorical(prob).sample().numpy()\n",
    "            s_prime, r, done, info = env.step(a)\n",
    "            s = s_prime\n",
    "            score += r\n",
    "        done = False\n",
    "    print(f\"Step # :{step_idx}, avg score : {score/num_test:.1f}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "# computing TD target\n",
    "def compute_target(v_final, r_lst, mask_lst):\n",
    "    G = v_final.reshape(-1)\n",
    "    td_target = list()\n",
    "\n",
    "    for r, mask in zip(r_lst[::-1], mask_lst[::-1]):\n",
    "        G = r + gamma * G * mask\n",
    "        td_target.append(G)\n",
    "\n",
    "    return torch.tensor(td_target[::-1]).float()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    envs = ParallelEnv(n_train_processes)\n",
    "\n",
    "    model = ActorCritic()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    step_idx = 0\n",
    "    # experience collection\n",
    "    s = envs.reset()\n",
    "    while step_idx < max_train_steps:\n",
    "        s_lst, a_lst, r_lst, mask_lst = list(), list(), list(), list()\n",
    "        for _ in range(update_interval):\n",
    "            prob = model.pi(torch.from_numpy(s).float())\n",
    "            a = Categorical(prob).sample().numpy()\n",
    "            s_prime, r, done, info = envs.step(a)\n",
    "\n",
    "            s_lst.append(s)\n",
    "            a_lst.append(a)\n",
    "            r_lst.append(r/100.0)\n",
    "            mask_lst.append(1 - done)\n",
    "\n",
    "            s = s_prime\n",
    "            step_idx += 1\n",
    "\n",
    "        s_final = torch.from_numpy(s_prime).float()\n",
    "        v_final = model.v(s_final).detach().clone().numpy()\n",
    "        td_target = compute_target(v_final, r_lst, mask_lst)\n",
    "\n",
    "        td_target_vec = td_target.reshape(-1)\n",
    "        s_vec = torch.tensor(s_lst).float().reshape(-1, 4)  # 4 == Dimension of state\n",
    "        a_vec = torch.tensor(a_lst).reshape(-1).unsqueeze(1)\n",
    "        advantage = td_target_vec - model.v(s_vec).reshape(-1) # TD target - estimated state-value\n",
    "\n",
    "        pi = model.pi(s_vec, softmax_dim=1)\n",
    "        pi_a = pi.gather(1, a_vec).reshape(-1)\n",
    "        loss = -(torch.log(pi_a) * advantage.detach()).mean() +\\\n",
    "            F.smooth_l1_loss(model.v(s_vec).reshape(-1), td_target_vec)\n",
    "\n",
    "        # performs gradient update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step_idx % PRINT_INTERVAL == 0:\n",
    "            test(step_idx, model)\n",
    "\n",
    "    envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068524cd-9a3c-4594-942c-f378621cbbde",
   "metadata": {},
   "source": [
    "### 9. SAC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29da06b-a977-4587-aa3b-0779200beaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import collections, random\n",
    "\n",
    "# a model-free RL algorithm designed for \n",
    "# continuous action spaces\n",
    "# Key features\n",
    "\n",
    "# 1. entropy regularization: encourages exploration by maximizing entropy in the policy\n",
    "# 2. twin q-networks; addresses overestimation bias by training two q-networks and using the minimum Q-value for updates\n",
    "# 3. target networks; stabilizes training using soft target updates\n",
    "# 4. automatic temperature tuning : adjusts entropy weight dynamically\n",
    "#Hyperparameters\n",
    "lr_pi           = 0.0005\n",
    "lr_q            = 0.001\n",
    "init_alpha      = 0.01\n",
    "gamma           = 0.98\n",
    "batch_size      = 32\n",
    "buffer_limit    = 5000\n",
    "tau             = 0.01 # for target network soft update\n",
    "target_entropy  = -1.0 # for automated alpha update\n",
    "lr_alpha        = 0.001  # for automated alpha update\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask = 0.0 if done else 1.0 \n",
    "            done_mask_lst.append([done_mask])\n",
    "        \n",
    "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst, dtype=torch.float), \\\n",
    "                torch.tensor(r_lst, dtype=torch.float), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                torch.tensor(done_mask_lst, dtype=torch.float)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, learning_rate):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 128)\n",
    "        self.fc_mu = nn.Linear(128,1)\n",
    "        self.fc_std  = nn.Linear(128,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        # automatic temperature tuning\n",
    "        self.log_alpha = torch.tensor(np.log(init_alpha))\n",
    "        self.log_alpha.requires_grad = True # entropy coeff automatically adjusted using gradient descent\n",
    "        self.log_alpha_optimizer = optim.Adam([self.log_alpha], lr=lr_alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        std = F.softplus(self.fc_std(x))\n",
    "        dist = Normal(mu, std) # outputs a stochastic action; normal distribution\n",
    "        action = dist.rsample() # reparameterization trick for differentiable sampling\n",
    "        log_prob = dist.log_prob(action)\n",
    "        real_action = torch.tanh(action)\n",
    "        real_log_prob = log_prob - torch.log(1-torch.tanh(action).pow(2) + 1e-7) # corrects log prob\n",
    "        return real_action, real_log_prob\n",
    "\n",
    "    def train_net(self, q1, q2, mini_batch):\n",
    "        s, _, _, _, _ = mini_batch\n",
    "        a, log_prob = self.forward(s)\n",
    "        entropy = -self.log_alpha.exp() * log_prob\n",
    "\n",
    "        q1_val, q2_val = q1(s,a), q2(s,a)\n",
    "        # uses twin Q-networks to prevent overestimations\n",
    "        q1_q2 = torch.cat([q1_val, q2_val], dim=1)\n",
    "        # uses min Q-value to update the policy\n",
    "        min_q = torch.min(q1_q2, 1, keepdim=True)[0]\n",
    "\n",
    "        # performs gradient ascent to maximize expected reward and entropy\n",
    "        loss = -min_q - entropy # for gradient ascent\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # updating alpha\n",
    "        # encourages policy to explore more when entropy is too low\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss = -(self.log_alpha.exp() * (log_prob + target_entropy).detach()).mean()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, learning_rate):\n",
    "        super(QNet, self).__init__()\n",
    "        self.fc_s = nn.Linear(3, 64)\n",
    "        self.fc_a = nn.Linear(1,64)\n",
    "        self.fc_cat = nn.Linear(128,32)\n",
    "        self.fc_out = nn.Linear(32,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        h1 = F.relu(self.fc_s(x))\n",
    "        h2 = F.relu(self.fc_a(a))\n",
    "        cat = torch.cat([h1,h2], dim=1) # concat state&action embeddings\n",
    "        q = F.relu(self.fc_cat(cat))\n",
    "        q = self.fc_out(q)\n",
    "        return q\n",
    "\n",
    "    def train_net(self, target, mini_batch):\n",
    "        s, a, r, s_prime, done = mini_batch\n",
    "        loss = F.smooth_l1_loss(self.forward(s, a) , target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    # updates target q-networks smoothly\n",
    "    def soft_update(self, net_target):\n",
    "        for param_target, param in zip(net_target.parameters(), self.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "def calc_target(pi, q1, q2, mini_batch):\n",
    "    s, a, r, s_prime, done = mini_batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # computes SAC target value\n",
    "        a_prime, log_prob= pi(s_prime)\n",
    "        entropy = -pi.log_alpha.exp() * log_prob\n",
    "        q1_val, q2_val = q1(s_prime,a_prime), q2(s_prime,a_prime)\n",
    "        q1_q2 = torch.cat([q1_val, q2_val], dim=1)\n",
    "        min_q = torch.min(q1_q2, 1, keepdim=True)[0]\n",
    "        target = r + gamma * done * (min_q + entropy)\n",
    "\n",
    "    return target\n",
    "    \n",
    "def main():\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    memory = ReplayBuffer()\n",
    "    q1, q2, q1_target, q2_target = QNet(lr_q), QNet(lr_q), QNet(lr_q), QNet(lr_q)\n",
    "    pi = PolicyNet(lr_pi)\n",
    "\n",
    "    q1_target.load_state_dict(q1.state_dict())\n",
    "    q2_target.load_state_dict(q2.state_dict())\n",
    "\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "\n",
    "    for n_epi in range(200):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        count = 0\n",
    "\n",
    "        while count < 200 and not done:\n",
    "            a, log_prob= pi(torch.from_numpy(s).float())\n",
    "            s_prime, r, done, truncated, info = env.step([2.0*a.item()])\n",
    "            memory.put((s, a.item(), r/10.0, s_prime, done))\n",
    "            score +=r\n",
    "            s = s_prime\n",
    "            count += 1\n",
    "                \n",
    "        if memory.size()>200:\n",
    "            for i in range(20):\n",
    "                mini_batch = memory.sample(batch_size)\n",
    "                td_target = calc_target(pi, q1_target, q2_target, mini_batch)\n",
    "                q1.train_net(td_target, mini_batch)\n",
    "                q2.train_net(td_target, mini_batch)\n",
    "                entropy = pi.train_net(q1, q2, mini_batch)\n",
    "                q1.soft_update(q1_target)\n",
    "                q2.soft_update(q2_target)\n",
    "                \n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f} alpha:{:.4f}\".format(n_epi, score/print_interval, pi.log_alpha.exp()))\n",
    "            score = 0.0\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ac77fd-9825-4d1a-97cc-2edc2a3dcb82",
   "metadata": {},
   "source": [
    "### 10. PPO continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9020cfa8-6349-4ed7-b7c7-9ddead8d9864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate  = 0.0003\n",
    "gamma           = 0.9\n",
    "lmbda           = 0.9\n",
    "eps_clip        = 0.2\n",
    "K_epoch         = 10\n",
    "rollout_len    = 3\n",
    "buffer_size    = 10\n",
    "minibatch_size = 32\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        self.fc1   = nn.Linear(3,128)\n",
    "        self.fc_mu = nn.Linear(128,1)\n",
    "        self.fc_std  = nn.Linear(128,1)\n",
    "        self.fc_v = nn.Linear(128,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.optimization_step = 0\n",
    "\n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = 2.0*torch.tanh(self.fc_mu(x))\n",
    "        std = F.softplus(self.fc_std(x))\n",
    "        return mu, std\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "      \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        s_batch, a_batch, r_batch, s_prime_batch, prob_a_batch, done_batch = [], [], [], [], [], []\n",
    "        data = []\n",
    "\n",
    "        for j in range(buffer_size):\n",
    "            for i in range(minibatch_size):\n",
    "                rollout = self.data.pop()\n",
    "                s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
    "\n",
    "                for transition in rollout:\n",
    "                    s, a, r, s_prime, prob_a, done = transition\n",
    "                    \n",
    "                    s_lst.append(s)\n",
    "                    a_lst.append([a])\n",
    "                    r_lst.append([r])\n",
    "                    s_prime_lst.append(s_prime)\n",
    "                    prob_a_lst.append([prob_a])\n",
    "                    done_mask = 0 if done else 1\n",
    "                    done_lst.append([done_mask])\n",
    "\n",
    "                s_batch.append(s_lst)\n",
    "                a_batch.append(a_lst)\n",
    "                r_batch.append(r_lst)\n",
    "                s_prime_batch.append(s_prime_lst)\n",
    "                prob_a_batch.append(prob_a_lst)\n",
    "                done_batch.append(done_lst)\n",
    "                    \n",
    "            mini_batch = torch.tensor(s_batch, dtype=torch.float), torch.tensor(a_batch, dtype=torch.float), \\\n",
    "                          torch.tensor(r_batch, dtype=torch.float), torch.tensor(s_prime_batch, dtype=torch.float), \\\n",
    "                          torch.tensor(done_batch, dtype=torch.float), torch.tensor(prob_a_batch, dtype=torch.float)\n",
    "            data.append(mini_batch)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def calc_advantage(self, data):\n",
    "        data_with_adv = []\n",
    "        for mini_batch in data:\n",
    "            s, a, r, s_prime, done_mask, old_log_prob = mini_batch\n",
    "            with torch.no_grad():\n",
    "                td_target = r + gamma * self.v(s_prime) * done_mask\n",
    "                delta = td_target - self.v(s)\n",
    "            delta = delta.numpy()\n",
    "\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "            data_with_adv.append((s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage))\n",
    "\n",
    "        return data_with_adv\n",
    "\n",
    "        \n",
    "    def train_net(self):\n",
    "        if len(self.data) == minibatch_size * buffer_size:\n",
    "            data = self.make_batch()\n",
    "            data = self.calc_advantage(data)\n",
    "\n",
    "            for i in range(K_epoch):\n",
    "                for mini_batch in data:\n",
    "                    s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage = mini_batch\n",
    "\n",
    "                    mu, std = self.pi(s, softmax_dim=1)\n",
    "                    dist = Normal(mu, std)\n",
    "                    log_prob = dist.log_prob(a)\n",
    "                    ratio = torch.exp(log_prob - old_log_prob)  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "                    # clipped PPO objective\n",
    "                    surr1 = ratio * advantage\n",
    "                    surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "                    loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.mean().backward()\n",
    "                    nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
    "                    self.optimizer.step()\n",
    "                    self.optimization_step += 1\n",
    "        \n",
    "def main():\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    model = PPO()\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "    rollout = []\n",
    "\n",
    "    for n_epi in range(10000):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        count = 0\n",
    "        while count < 200 and not done:\n",
    "            for t in range(rollout_len):\n",
    "                mu, std = model.pi(torch.from_numpy(s).float())\n",
    "                dist = Normal(mu, std)  # uses stochastic policy for action selection\n",
    "                a = dist.sample()\n",
    "                log_prob = dist.log_prob(a) # stores log-probabilities for PPO updates\n",
    "                s_prime, r, done, truncated, info = env.step([a.item()])\n",
    "\n",
    "                # stores rollouts steps at a time\n",
    "                rollout.append((s, a, r/10.0, s_prime, log_prob.item(), done))\n",
    "                if len(rollout) == rollout_len:\n",
    "                    model.put_data(rollout)\n",
    "                    rollout = []\n",
    "\n",
    "                s = s_prime\n",
    "                score += r\n",
    "                count += 1\n",
    "\n",
    "            model.train_net()\n",
    "\n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}, optmization step: {}\".format(n_epi, score/print_interval, model.optimization_step))\n",
    "            score = 0.0\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "# Key features\n",
    "# 1. uses clipped objective function to ensure stable policy updates\n",
    "# 2. advantage estimation\n",
    "# 3. stochastic policy (uses gaussian policies for continuous control)\n",
    "# 4. gradient clipping \n",
    "# 5. batch updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3f2c8-8ca7-487c-afd1-185e9e318376",
   "metadata": {},
   "source": [
    "### 11. V traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0758d0c1-dbf7-4837-954a-80286d1f53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate      = 0.0005\n",
    "gamma              = 0.98\n",
    "T_horizon          = 20\n",
    "clip_rho_threshold = 1.0\n",
    "clip_c_threshold   = 1.0\n",
    "print_interval     = 20\n",
    "\n",
    "class Vtrace(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vtrace, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        self.fc1   = nn.Linear(4,256)\n",
    "        self.fc_pi = nn.Linear(256,2)\n",
    "        self.fc_v  = nn.Linear(256,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.clip_rho_threshold = torch.tensor(clip_rho_threshold, dtype=torch.float)\n",
    "        self.clip_c_threshold = torch.tensor(clip_c_threshold, dtype=torch.float)\n",
    "\n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=softmax_dim)\n",
    "        return prob\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "      \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, mu_a_lst, done_lst = [], [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            s, a, r, s_prime, mu_a, done = transition\n",
    "            \n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            mu_a_lst.append([mu_a])\n",
    "            done_mask = 0 if done else 1\n",
    "            done_lst.append([done_mask])\n",
    "            \n",
    "        s,a,r,s_prime,done_mask, mu_a = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "                                        torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                                        torch.tensor(done_lst, dtype=torch.float), torch.tensor(mu_a_lst)\n",
    "        self.data = []\n",
    "        return s, a, r, s_prime, done_mask, mu_a\n",
    "\n",
    "    def vtrace(self, s, a, r, s_prime, done_mask, mu_a):\n",
    "        with torch.no_grad():\n",
    "            pi = self.pi(s, softmax_dim=1)\n",
    "            pi_a = pi.gather(1,a)\n",
    "            v, v_prime = self.v(s), self.v(s_prime)\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(mu_a))  # a/b == exp(log(a)-log(b))\n",
    "            \n",
    "            rhos = torch.min(self.clip_rho_threshold, ratio)\n",
    "            cs = torch.min(self.clip_c_threshold, ratio).numpy()\n",
    "            td_target = r + gamma * v_prime * done_mask\n",
    "            delta = rhos*(td_target - v).numpy()\n",
    "            \n",
    "            vs_minus_v_xs_lst = []\n",
    "            vs_minus_v_xs = 0.0\n",
    "            vs_minus_v_xs_lst.append([vs_minus_v_xs])\n",
    "            \n",
    "            for i in range(len(delta)-1, -1, -1):\n",
    "                vs_minus_v_xs = gamma * cs[i][0] * vs_minus_v_xs + delta[i][0]\n",
    "                vs_minus_v_xs_lst.append([vs_minus_v_xs])\n",
    "            vs_minus_v_xs_lst.reverse()\n",
    "            \n",
    "            vs_minus_v_xs = torch.tensor(vs_minus_v_xs_lst, dtype=torch.float)\n",
    "            vs = vs_minus_v_xs[:-1] + v.numpy()\n",
    "            vs_prime = vs_minus_v_xs[1:] + v_prime.numpy()\n",
    "            advantage = r + gamma * vs_prime - v.numpy()\n",
    "            \n",
    "        return vs, advantage, rhos\n",
    "\n",
    "    def train_net(self):\n",
    "        s, a, r, s_prime, done_mask, mu_a = self.make_batch()\n",
    "        vs, advantage, rhos = self.vtrace(s, a, r, s_prime, done_mask, mu_a)\n",
    "\n",
    "        pi = self.pi(s, softmax_dim=1)\n",
    "        pi_a = pi.gather(1,a)\n",
    "       \n",
    "        val_loss = F.smooth_l1_loss(self.v(s) , vs)\n",
    "        pi_loss = -rhos * torch.log(pi_a) * advantage\n",
    "        loss =  pi_loss + val_loss\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    model = Vtrace()\n",
    "    score = 0.0\n",
    "    \n",
    "    for n_epi in range(10000):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            for t in range(T_horizon):\n",
    "                prob = model.pi(torch.from_numpy(s).float())\n",
    "                m = Categorical(prob)\n",
    "                a = m.sample().item()\n",
    "                s_prime, r, done, truncated, info = env.step(a)\n",
    "\n",
    "                model.put_data((s, a, r/100.0, s_prime, prob[a].item(), done))\n",
    "                s = s_prime\n",
    "\n",
    "                score += r\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            model.train_net()\n",
    "\n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
    "            score = 0.0\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
