{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6870892-a6f1-452b-ac21-1f982c7ff44e",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this blog post, Andrej Karpathy explores the power and versatility of Recurrent Neural Networks (RNNs) for sequence modeling tasks. He demonstrates how RNNs can learn complex temporal dependencies and generate coherent sequences across various domains, including text, music, and programming code. The article emphasizes the power of RNNs to model diverse data distributions with minimal feature engineering. Through engaging examples, Karpathy showcases how character-level RNNs can generate creative and plausible outputs that mimic the style of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16be540-0d77-44ff-b7c1-344af0a6f686",
   "metadata": {},
   "source": [
    "# Key Contributions\n",
    "\n",
    "1. Practical Demonstration of RNNs\n",
    "   * The article presents accessible and intuitive examples of RNN's capabilities, such as generating Shakespeare-like text, LaTeX documents, and programming code.\n",
    "   * It highlights the practical steps for training and fine tuning RNNs on sequence data.\n",
    "\n",
    "2. Character-Level Modeling:\n",
    "   * Karpathy demonstrates the effectiveness of character-level RNNs, where the model learns to predict the next character in a sequence. This approach eliminates the need for explicit tokenization, making it widely applicable across domains.\n",
    "\n",
    "3. Diverse Applications:\n",
    "   * The article provides examples of RNN applications in text, music, and code generation, illustrating the broad applicablity of the architecture.\n",
    "  \n",
    "4. Visualization of RNN Internals:\n",
    "   * A notable contribution is the visualization of RNN hiddle states, providing insights into how the model learns patterns such as balanced parentheses or sentence structures.\n",
    "  \n",
    "5. Advocacy for Simplicity:\n",
    "   * Karpathy emphasizes the power of simple architectures (e.g., vanilla RNNs with minimal preprocessing) to achieve remarkable results, inspiring practitioners to experiment with straightforward setups before considering more complex architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea087577-e8b0-440b-9563-52c5b1b0de7f",
   "metadata": {},
   "source": [
    "# Technical Details\n",
    "\n",
    "* RNNs process sequences: Unlike Vanilla Neural Networks that take a fixed-size vector as input and produce a fixed-size vector as output, RNNs operate over sequences of vectors. These sequences can be in the input, the output, or both.\n",
    "* RNNs maintain a hidden state: RNNs have an internal state, typically represented by a hidden vector $h$, which gets updated every time a new input is processed. The output of the RNN is influenced not only bt the current input but also by the history of inputs.\n",
    "* The step function: The core API of an RNN involves a step function that takes an input vector x and returns an output vector y, while updating its internal hidden state. A basic implementation of the step function in a Vanilla RNN is: self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x)). The parameters of the RNN include the matrices W_hh, W_xh, and W_hy, which are learned during training.\n",
    "* Deep RNNs: RNNs can be stacked to create deeper models, where the output of one RNN serves as the input to the next.\n",
    "* Long Short-Term Memory (LSTM): The article mentions that, in practice, a variation called the LSTM is often useed. LSTMs have a more complex update equation, but maintain the same over structure as RNNs.\n",
    "* Character-level language models: RNNs can be trained to predict the next character in a sequence, given the previous characters. For this, each character is encoded into a vector using 1-of-k encoding and fed into the RNN. The RNN outputs a vector that is interpreted as the confidence the model has for each character as the next in the sequence.\n",
    "* Training process: The RNN is trained using backpropagation to adjust the weights to increase the scores for the correct target characters. A softmax classifier is used on every output vector and the model is trianed with mini-batch Stochastic Gradient Descent, using methods like RMSProp or Adam to stablilze updates.\n",
    "* Sampling: At test time, the RNN's output is a probability distribution over what characters are likely to come next; a character is sampled from the distribution and fed back into the RNN to get the next letter.\n",
    "* Temperature: The temperature\" of the Softmax can be adjusted during sampling. Lower temperatures make the RNN more confident but also more conservative, while higher temperatures increase diversity at the cost of more mistakes.\n",
    "* Visualization: The article discusses visualizations to understand the model, including visualizing the firing of neurons in the hidden representation and the model's confidence about the next characters. Some neurons learn to keep track of things like whether the model is inside a URL or markdown environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d7b295-b5a9-4705-91a5-d5886fa6bf63",
   "metadata": {},
   "source": [
    "# Potential Research Directions\n",
    "\n",
    "1. Improving RNN Architectures: explore improvements over vanilla RNNs to address their limitations, such as vanishing gradients. This has already led to innovations like LSTMs, GRUs and Transformer-based architectures.\n",
    "2. Sequence Modeling Beyond Characters: Investigate how RNNs can be applied to sequence modeling at higher levels, such as word- or sentence-level modeling, for more sophisticated tasks like machine translation.\n",
    "3. Combining RNNs with Attention: While the article predates Transformers, one research direction involves combining RNNs with attention mechanisms for tasks requiring long-term dependencies.\n",
    "4. Creative Generative Applications: Expand n the creative applications demonstrated in the article, such as using RNNs for procedural content generation in video games or interactive storytelling.\n",
    "5. Explainability and Visualization: Build on Karpathyâ€™s work in visualizing RNN hidden states to better understand what and how these models learn, enabling interpretable deep learning systems.\n",
    "6. Cross-Domain Sequence Learning: Apply the ideas of character-level modeling to new domains, such as biological sequences (e.g., DNA/RNA) or financial time series, to discover novel applications.\n",
    "7. Comparative Studies with Modern Architectures: Compare RNN's performance with modern sequence modeling architectures like Transformeres to identify scenarios where RNNs remain competitive or preferable.\n",
    "8. Low-Resource and Edge Applications:Investigate the use of RNNs in resource-constrained environments, where their smaller memory footprint and simplicity could be advantageous over larger models like Transformers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace63e1b-342c-43e1-a463-ebafe6f53a10",
   "metadata": {},
   "source": [
    "# Tips and Tricks\n",
    "\n",
    "**Monitoring Validation Loss vs. Training Loss**\n",
    "\n",
    "The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "* If your training loss is much lower than validation loss then this means the network might be overfitting. Solutions to this are to decrease your network size, to to increase dropout. For example, you could try dropout of 0.5 and so on.\n",
    "* If your training/validation loss are about equal then your model is underfitting. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "**Approximate number of parameters**\n",
    "\n",
    "The two most important parameters that control the model are _rnn_size_ and _num_layers_. I would advice that you always use _num_layers_ of either 2 or 3. The _rnn_size_ can be adjusted based on how much data you have. The two important quatities to keep track of here are:\n",
    "* The number of parameters in your model. This is printed when you start training.\n",
    "* The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    "These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "* I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make _rnn_size_ larger.\n",
    "* I have a 10MB dataset and running a 10 million parameter model. I am slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my traininig loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "**Best Models Strategy**\n",
    "\n",
    "The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0, 1). Whatever model has the best validation performance (the loss, low is good) is the one you should use in the end. \n",
    "\n",
    "It's very common in deep leraning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance. \n",
    "\n",
    "By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e0713-fe3b-4dda-911c-b5b4e87e9e97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
