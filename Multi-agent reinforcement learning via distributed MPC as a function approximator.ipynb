{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dae000d-27b8-4a02-a73e-553abeeaf1d4",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1bb867-a852-4e7e-ac38-2b5cb4f1d714",
   "metadata": {},
   "source": [
    "* The integration of MPC and RL is a promising direction for achieving safe and intepretable learning-based control. In particular, MPC has been proposed as a replacement for DNN function approximators in RL. In this context, the optimal control and cost of the MPC  optimization problem represent the policy and value function respectively.\n",
    "* Gap 1: Past method relies on a centralized approach with a single learning agent. This is in general prohibitive for multi-agent systems, where 1. centralization requires either a specific topology, with all agents connected to the central agent, or 2. multi-hop communication across intermediate agents, where the number of hops grows with the network size.\n",
    "* Solution 1: Addressing these challenges, distributed control of multi-agent system offers computational scalability and privacy, with only neighbor-to-neighbor communication. Many existing works have adapted the MPC methodology to the distributed setting with distributed MPC and, likewise, RL to the multi-agent setting RL (MARL) setting.\n",
    "* Gap 2: A central challenge in MARL is that the interaction of simultaneuously learning agents renders the learning target of each agent nonstationary, degrading the performance and convergence properties of single agent algorithms.\n",
    "* Solution 2: Several works have tried to circumvent this problem using a centralized training and decentralized execution paradigm. However, centralized training is often either unrealistic or unavailable. 1. Soome approaches address the nonstationarity issue through communication across the network of agents during learning. These works provide theoretical convergence guarantees, but focus on linear function approximation. 2. MARL has also been addressed with DNN function approximators but these approaches do not emphasize information exchange between agents, and suffer from the same drawbacks as DNN-based single agent RL. => **Addressing the nonstationarity of learning targets in MARL remains an open challenge**\n",
    "* Contribution of this paper:\n",
    "  * The use of MPC as a function approximator in RL is extended to the multiple agent setting. => propose a structured convex distributed MPC scheme as an approximator for the policy and value functions, introducing a novel, model-based MARL approach for linear systems, free from nonstationary.\n",
    "  * The method is distributed in training and deployment, with data sharing only between neighbors, irrespective of the network size and topology, thus avoiding centralized computation and multi-hop data communication.\n",
    "  * Privacy of sensitive information in local parameters and functions is preserved, with only state trajectories being shared, in contrast to a centralized approach where local functions are shared with the central agent.\n",
    "  * MPC-based approximation => insights into the policy can be gained from the learned components (prediction model and constraints). Also, in contrast to DNN-based approaches, it is possible to inject a priori information like model approximations. \n",
    "  * cosensus optimization: relating the dual variables recovered distributively through the alternating direction method of multipliers (ADMM) to the optimal dual variables of the original problem, which enables the distributed learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab9efd6-04f1-44f2-b933-fa86fdd097d3",
   "metadata": {},
   "source": [
    "# MATH (page 13-23)\n",
    "\n",
    "https://share.goodnotes.com/s/rNnfc98YT7yQX6UEuv3LzI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65174a-4e55-4530-9da9-c1af22c25fc1",
   "metadata": {},
   "source": [
    "# Code for Algorithm 1 in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cc520f8-6e31-4095-b92d-82550e1cfe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and System Setup\n",
    "\n",
    "# Constants and Flags:\n",
    "CENTRALISED = False # use a distributed setup (False: decentralized agents)\n",
    "LEARN = True # enable learning mode\n",
    "USE_LEARNED_PARAMS = False # use pre-learned parameters\n",
    "STORE_DATA = True # save results\n",
    "PLOT = False # disable plotting\n",
    "\n",
    "# System parameters\n",
    "\n",
    "(n, # number of agents\n",
    " nx_l, # number of state variables per agent\n",
    " nu_l, # adjacency matrix\n",
    " Adj, # time step for the simulation\n",
    " ts,  # time step for the simulation\n",
    " prediction_length, # horizon for MPC\n",
    " discount_factor, # RL discounting for cost function\n",
    " u_lim, theta_lim, # Control and state constraints\n",
    " w, # weight for penalties in cost functiona\n",
    " load_noise_bnd, # noise bound for stochastic disturbances\n",
    ") = get_model_details()\n",
    "\n",
    "# ADMM concensus matrix\n",
    "# facilitate communication between neighboring agents\n",
    "G = g_map(Adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49642853-2e65-4a17-8383-fd9aacb9abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Distributed MPC (local MPC):\n",
    "# Each Local MPC agent solves its optimization problem with ADMM consensus constraints\n",
    "\n",
    "# Key parameters\n",
    "class LocalMpc(MpcAdmm):\n",
    "    rho = 50 # ADMM penalty parameter\n",
    "    horizon = prediction_length # MPC prediction horizon\n",
    "    to_learn = [\"theta_lb\", \"theta_ub\", \"V0\", \"b\", \"f_x\", \"f_u\", \"Q_x\", \"Q_u\"]\n",
    "\n",
    "    # initialization\n",
    "    def __init__(self, num_neighbours, my_index, pars_init, P_tie_init, u_lim):\n",
    "        \"\"\"\n",
    "        num_neighbours: number of connected regions\n",
    "        my_index: agent's index with G for local computations\n",
    "        pars_init: initial model parameters\n",
    "        P_tie_init: Initial power tie-line values\n",
    "        u_lim: Control limits\n",
    "        \"\"\"\n",
    "        # add coupling to learn\n",
    "        for i in range(num_neighbours):\n",
    "            self.to_learn = self.to_learn + [f\"P_tie_{i}\"]\n",
    "\n",
    "        N = self.horizon\n",
    "        gamma = discount_factor\n",
    "        nlp = Nlp[cs.SX]()\n",
    "        super().__init__(nlp, N)\n",
    "\n",
    "        # model paramters\n",
    "        self.learnable_pars_init = {}\n",
    "        self.fixed_pars_init[\"load\"] = np.zeros((n, 1))\n",
    "        self.fixed_pars_init[\"x_o\"] = np.zeros((n * nx_l, 1))\n",
    "        self.fixed_pars_init[\"u_o\"] = np.zeros((n * nu_l, 1))\n",
    "        \n",
    "        for name, val in pars_init.items():\n",
    "            if name in self.to_learn:\n",
    "                self.learnable_pars_init[name] = val\n",
    "            else:\n",
    "                self.fixed_pars_init[name] = val\n",
    "        for i in range(num_neighbours):\n",
    "            if f\"P_tie_{i}\" in self.to_learn:\n",
    "                self.learnable_pars_init[f\"P_tie_{i}\"] = P_tie_init[i]\n",
    "            else:\n",
    "                self.fixed_pars_init[f\"P_tie_{i}\"] = P_tie_init[i]\n",
    "\n",
    "        # learnable parameters setup\n",
    "        H = self.parameter(\"H\", (1,))\n",
    "        R = self.parameter(\"R\", (1,))\n",
    "        D = self.parameter(\"D\", (1,))\n",
    "        T_t = self.parameter(\"T_t\", (1,))\n",
    "        T_g = self.parameter(\"T_g\", (1,))\n",
    "        \n",
    "        # coupling parameters for neighboring agents\n",
    "        P_tie_list = [] # stores power transfer coefficients between agents\n",
    "        for i in range(num_neighbours):\n",
    "            P_tie_list.append(self.parameter(f\"P_tie_{i}\", (1,)))\n",
    "\n",
    "        theta_lb = self.parameter(f\"theta_lb\", (1,))\n",
    "        theta_ub = self.parameter(f\"theta_ub\", (1,))\n",
    "\n",
    "        V0 = self.parameter(f\"V0\", (1,))\n",
    "        b = self.parameter(f\"b\", (nx_l,))\n",
    "        f_x = self.parameter(f\"f_x\", (nx_l,))\n",
    "        f_u = self.parameter(f\"f_u\", (nu_l,))\n",
    "        Q_x = self.parameter(f\"Q_x\", (nx_l, nx_l))\n",
    "        Q_u = self.parameter(f\"Q_u\", (nu_l, nu_l))\n",
    "\n",
    "        load = self.parameter(\"load\", (1, 1))\n",
    "        x_o = self.parameter(\"x_o\", (nx_l, 1))\n",
    "        u_o = self.parameter(\"u_o\", (nu_l, 1))\n",
    "\n",
    "        A, B, L, A_c_list = get_learnable_dynamics_local(H, R, D, T_t, T_g, P_tie_list)\n",
    "\n",
    "        # state and control variables\n",
    "        # agent's state x and coupled states x_c\n",
    "        x, x_c = self.augmented_state(num_neighbours, my_index, size=nx_l)\n",
    "        # control action with limits\n",
    "        u, _ = self.action(\n",
    "            \"u\",\n",
    "            nu_l,\n",
    "            lb=-u_lim,\n",
    "            ub=u_lim\n",
    "        )\n",
    "        s, _, _ = self.variable(\"s\", (1, N), lb=0)  # dim 1 as only theta has bound\n",
    "        \n",
    "        x_c_list = ([])  # store the bits of x that are couplings in a list for ease of access\n",
    "        for i in range(num_neighbours):\n",
    "            x_c_list.append(x_c[nx_l * i : nx_l * (i + 1), :])\n",
    "\n",
    "        # dynamics - added manually due to coupling\n",
    "        for k in range(N):\n",
    "            coup = cs.SX.zeros(nx_l, 1)\n",
    "            for i in range(num_neighbours):  # get coupling expression\n",
    "                coup += A_c_list[i] @ x_c_list[i][:, [k]]\n",
    "            self.constraint(\n",
    "                \"dynam_\" + str(k),\n",
    "                A @ x[:, [k]] + B @ u[:, [k]] + L @ load + coup + b_scaling * b,\n",
    "                \"==\",\n",
    "                x[:, [k + 1]],\n",
    "            )\n",
    "\n",
    "        # MPC constraints\n",
    "        self.constraint(f\"theta_lb\", -theta_lim + theta_lb - s, \"<=\", x[0, 1:])\n",
    "        self.constraint(f\"theta_ub\", x[0, 1:], \"<=\", theta_lim + theta_ub + s)\n",
    "\n",
    "        # objective / cost function\n",
    "        self.set_local_cost(\n",
    "                            V0 + sum(\n",
    "                                f_x.T @ x[:, k] + f_u.T @ u[:, k] + (gamma**k) * (\n",
    "                                    (x[:, k] - x_o).T @ Q_x @ (x[:, k] - x_o) +\n",
    "                                    (u[:, k] - u_o).T @ Q_u @ (u[:, k] - u_o) +\n",
    "                                    w_l * s[:, [k]]\n",
    "                                )\n",
    "                                for k in range(N)\n",
    "                            )\n",
    "                        )\n",
    "        self.nx_l = nx_l\n",
    "        self.nu_l = nu_l\n",
    "\n",
    "        # solver\n",
    "        opts = {\n",
    "            \"expand\": True, \"print_time\": False, \"calc_lam_x\": True,\n",
    "            \"ipopt\": {\"max_iter\": 2000, \"sb\": \"yes\", \"print_level\": 0}\n",
    "        }\n",
    "        self.init_solver(opts, solver=\"ipopt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d2f82e-a475-4fdc-a20d-d51e606c9878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralized MPC\n",
    "# if centralized = true, a single centralized MPC is used instead of multiple local agentsabs\n",
    "\n",
    "class CentralisedMpc(Mpc[cs.SX]):\n",
    "    \"\"\"The centralised MPC controller.\"\"\"\n",
    "\n",
    "    horizon = prediction_length\n",
    "\n",
    "    # define which params are learnable\n",
    "    to_learn = []\n",
    "    # to_learn = [f\"H_{i}\" for i in range(n)]\n",
    "    # to_learn = to_learn + [f\"R_{i}\" for i in range(n)]\n",
    "    # to_learn = to_learn + [f\"D_{i}\" for i in range(n)]\n",
    "    # to_learn = to_learn + [f\"T_t_{i}\" for i in range(n)]\n",
    "    # to_learn = to_learn + [f\"T_g_{i}\" for i in range(n)]\n",
    "    to_learn = to_learn + [f\"theta_lb_{i}\" for i in range(n)]\n",
    "    to_learn = to_learn + [f\"theta_ub_{i}\" for i in range(n)]\n",
    "    to_learn = to_learn + [f\"V0_{i}\" for i in range(n)]\n",
    "    to_learn = to_learn + [f\"b_{i}\" for i in range(n)]\n",
    "    to_learn = to_learn + [f\"f_x_{i}\" for i in range(n)]\n",
    "    to_learn = to_learn + [f\"f_u_{i}\" for i in range(n)]\n",
    "    to_learn = to_learn + [f\"Q_x_{i}\" for i in range(n)]\n",
    "    to_learn = to_learn + [f\"Q_u_{i}\" for i in range(n)]\n",
    "    to_learn = to_learn + [\n",
    "        f\"P_tie_{i}_{j}\" for j in range(n) for i in range(n) if Adj[i, j] == 1\n",
    "    ]\n",
    "\n",
    "    # initialise parameters vals\n",
    "\n",
    "    learnable_pars_init = {}\n",
    "    fixed_pars_init = {\n",
    "        \"load\": np.zeros((n, 1)),\n",
    "        \"x_o\": np.zeros((n * nx_l, 1)),\n",
    "        \"u_o\": np.zeros((n * nu_l, 1)),\n",
    "    }\n",
    "\n",
    "    # model params\n",
    "    if USE_LEARNED_PARAMS:\n",
    "        P_tie_init = get_learned_P_tie_init()\n",
    "        pars_init_list = get_learned_pars_init_list()\n",
    "    else:\n",
    "        P_tie_init = get_P_tie_init()\n",
    "        pars_init_list = get_pars_init_list()\n",
    "    for i in range(n):\n",
    "        for name, val in pars_init_list[i].items():\n",
    "            if f\"{name}_{i}\" in to_learn:\n",
    "                learnable_pars_init[f\"{name}_{i}\"] = val\n",
    "            else:\n",
    "                fixed_pars_init[f\"{name}_{i}\"] = val\n",
    "\n",
    "    # coupling params\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if Adj[i, j] == 1:\n",
    "                if f\"P_tie_{i}_{j}\" in to_learn:\n",
    "                    learnable_pars_init[f\"P_tie_{i}_{j}\"] = P_tie_init[i, j]\n",
    "                else:\n",
    "                    fixed_pars_init[f\"P_tie_{i}_{j}\"] = P_tie_init[i, j]\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        N = self.horizon\n",
    "        gamma = discount_factor\n",
    "        nlp = Nlp[cs.SX]()\n",
    "        super().__init__(nlp, N)\n",
    "\n",
    "        # init params\n",
    "\n",
    "        H_list = [self.parameter(f\"H_{i}\", (1,)) for i in range(n)]\n",
    "        R__list = [self.parameter(f\"R_{i}\", (1,)) for i in range(n)]\n",
    "        D_list = [self.parameter(f\"D_{i}\", (1,)) for i in range(n)]\n",
    "        T_t_list = [self.parameter(f\"T_t_{i}\", (1,)) for i in range(n)]\n",
    "        T_g_list = [self.parameter(f\"T_g_{i}\", (1,)) for i in range(n)]\n",
    "        P_tie_list_list = []\n",
    "        for i in range(n):\n",
    "            P_tie_list_list.append([])\n",
    "            for j in range(n):\n",
    "                if Adj[i, j] == 1:\n",
    "                    P_tie_list_list[i].append(self.parameter(f\"P_tie_{i}_{j}\", (1,)))\n",
    "                else:\n",
    "                    P_tie_list_list[i].append(0)\n",
    "\n",
    "        A, B, L = get_learnable_dynamics(\n",
    "            H_list, R__list, D_list, T_t_list, T_g_list, P_tie_list_list\n",
    "        )\n",
    "\n",
    "        load = self.parameter(\"load\", (n, 1))\n",
    "        x_o = self.parameter(\"x_o\", (n * nx_l, 1))\n",
    "        u_o = self.parameter(\"u_o\", (n * nu_l, 1))\n",
    "\n",
    "        theta_lb = [self.parameter(f\"theta_lb_{i}\", (1,)) for i in range(n)]\n",
    "        theta_ub = [self.parameter(f\"theta_ub_{i}\", (1,)) for i in range(n)]\n",
    "\n",
    "        V0 = [self.parameter(f\"V0_{i}\", (1,)) for i in range(n)]\n",
    "        b = [self.parameter(f\"b_{i}\", (nx_l,)) for i in range(n)]\n",
    "        f_x = [self.parameter(f\"f_x_{i}\", (nx_l,)) for i in range(n)]\n",
    "        f_u = [self.parameter(f\"f_u_{i}\", (nu_l,)) for i in range(n)]\n",
    "        Q_x = [self.parameter(f\"Q_x_{i}\", (nx_l, nx_l)) for i in range(n)]\n",
    "        Q_u = [self.parameter(f\"Q_u_{i}\", (nu_l, nu_l)) for i in range(n)]\n",
    "\n",
    "        # mpc vars\n",
    "\n",
    "        x, _ = self.state(\"x\", n * nx_l)\n",
    "        u, _ = self.action(\"u\", n * nu_l, lb=-u_lim, ub=u_lim)\n",
    "        s, _, _ = self.variable(\n",
    "            \"s\",\n",
    "            (n, N),\n",
    "            lb=0,\n",
    "        )  # n in first dim as only cnstr on theta\n",
    "\n",
    "        # state constraints\n",
    "\n",
    "        for i in range(n):  # only a constraint on theta\n",
    "            for k in range(1, N):\n",
    "                self.constraint(\n",
    "                    f\"theta_lb_{i}_{k}\",\n",
    "                    -theta_lim + theta_lb[i] - s[i, k],\n",
    "                    \"<=\",\n",
    "                    x[i * nx_l, k],\n",
    "                )\n",
    "                self.constraint(\n",
    "                    f\"theta_ub_{i}_{k}\",\n",
    "                    x[i * nx_l, k],\n",
    "                    \"<=\",\n",
    "                    theta_lim + theta_ub[i] + s[i, k],\n",
    "                )\n",
    "\n",
    "        # dynamics\n",
    "\n",
    "        b_full = cs.SX()\n",
    "        for i in range(n):\n",
    "            b_full = cs.vertcat(b_full, b[i])\n",
    "        self.set_dynamics(\n",
    "            lambda x, u: A @ x + B @ u + L @ load + b_scaling * b_full, n_in=2, n_out=1\n",
    "        )\n",
    "\n",
    "        # objective\n",
    "\n",
    "        Q_x_full = cs.diagcat(*Q_x)\n",
    "        Q_u_full = cs.diagcat(*Q_u)\n",
    "\n",
    "        f_x_full = cs.SX()\n",
    "        f_u_full = cs.SX()\n",
    "        for i in range(n):\n",
    "            f_x_full = cs.vertcat(f_x_full, f_x[i])\n",
    "            f_u_full = cs.vertcat(f_u_full, f_u[i])\n",
    "\n",
    "        self.minimize(\n",
    "            sum(V0)\n",
    "            + sum(\n",
    "                f_x_full.T @ x[:, k]\n",
    "                + f_u_full.T @ u[:, k]\n",
    "                + (gamma**k)\n",
    "                * (\n",
    "                    (x[:, k] - x_o).T @ Q_x_full @ (x[:, k] - x_o)\n",
    "                    + (u[:, k] - u_o).T @ Q_u_full @ (u[:, k] - u_o)\n",
    "                    + w.T @ s[:, [k]]\n",
    "                )\n",
    "                for k in range(N)\n",
    "            )\n",
    "            + (gamma**N) * (x[:, N] - x_o).T @ Q_x_full @ (x[:, N] - x_o)\n",
    "        )\n",
    "\n",
    "        # solver\n",
    "        opts = {\n",
    "            \"expand\": True,\n",
    "            \"print_time\": False,\n",
    "            \"bound_consistency\": True,\n",
    "            \"calc_lam_x\": True,\n",
    "            \"calc_lam_p\": False,\n",
    "            # \"jit\": True,\n",
    "            # \"jit_cleanup\": True,\n",
    "            \"ipopt\": {\n",
    "                # \"linear_solver\": \"ma97\",\n",
    "                # \"linear_system_scaling\": \"mc19\",\n",
    "                # \"nlp_scaling_method\": \"equilibration-based\",\n",
    "                \"max_iter\": 1000,\n",
    "                \"sb\": \"yes\",\n",
    "                \"print_level\": 0,\n",
    "            },\n",
    "        }\n",
    "        self.init_solver(opts, solver=\"ipopt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c939b7-778e-49de-9957-9c905ded9fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL agent\n",
    "# implements least squares temporal difference Q-LEARNING (LSTD-Q)\n",
    "# overrides on_timestep_end() to update load values dynamically\n",
    "class LoadedLstdQLearningAgentCoordinator(LstdQLearningAgentCoordinator):\n",
    "    def on_timestep_end(self, env, episode: int, timestep: int) -> None:\n",
    "        self.fixed_parameters[\"load\"] = env.load\n",
    "        self.fixed_parameters[\"x_o\"] = env.x_o\n",
    "        self.fixed_parameters[\"u_o\"] = env.u_o\n",
    "\n",
    "        if not self.centralised_flag:\n",
    "            for i in range(n):\n",
    "                self.agents[i].fixed_parameters[\"load\"] = env.load[i]\n",
    "                self.agents[i].fixed_parameters[\"x_o\"] = env.x_o[\n",
    "                    nx_l * i : nx_l * (i + 1)\n",
    "                ]\n",
    "                self.agents[i].fixed_parameters[\"u_o\"] = env.u_o[i]\n",
    "\n",
    "        return super().on_timestep_end(env, episode, timestep)\n",
    "\n",
    "    def on_episode_start(self, env, episode: int) -> None:\n",
    "        self.fixed_parameters[\"load\"] = env.load\n",
    "        self.fixed_parameters[\"x_o\"] = env.x_o\n",
    "        self.fixed_parameters[\"u_o\"] = env.u_o\n",
    "\n",
    "        if not self.centralised_flag:\n",
    "            for i in range(n):\n",
    "                self.agents[i].fixed_parameters[\"load\"] = env.load[i]\n",
    "                self.agents[i].fixed_parameters[\"x_o\"] = env.x_o[\n",
    "                    nx_l * i : nx_l * (i + 1)\n",
    "                ]\n",
    "                self.agents[i].fixed_parameters[\"u_o\"] = env.u_o[i]\n",
    "\n",
    "        return super().on_episode_start(env, episode)\n",
    "\n",
    "\n",
    "# create distributed mpc's and parameters\n",
    "if USE_LEARNED_PARAMS:\n",
    "    P_tie_init = get_learned_P_tie_init()\n",
    "    pars_init_list = get_learned_pars_init_list()\n",
    "else:\n",
    "    P_tie_init = get_P_tie_init()\n",
    "    pars_init_list = get_pars_init_list()\n",
    "# distributed mpc and params\n",
    "mpc_dist_list: list[Mpc] = []\n",
    "learnable_dist_parameters_list: list[LearnableParametersDict] = []\n",
    "fixed_dist_parameters_list: list = []\n",
    "\n",
    "for i in range(n):\n",
    "    mpc_dist_list.append(\n",
    "        LocalMpc(\n",
    "            num_neighbours=len(G[i]) - 1,\n",
    "            my_index=G[i].index(i),\n",
    "            pars_init=pars_init_list[i],\n",
    "            P_tie_init=[P_tie_init[i, j] for j in range(n) if Adj[i, j] != 0],\n",
    "            u_lim=u_lim[i],\n",
    "        )\n",
    "    )\n",
    "    learnable_dist_parameters_list.append(\n",
    "        LearnableParametersDict[cs.SX](\n",
    "            (\n",
    "                LearnableParameter(\n",
    "                    name, val.shape, val, sym=mpc_dist_list[i].parameters[name]\n",
    "                )\n",
    "                for name, val in mpc_dist_list[i].learnable_pars_init.items()\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    fixed_dist_parameters_list.append(mpc_dist_list[i].fixed_pars_init)\n",
    "\n",
    "# create distributed mpc's and parameters\n",
    "mpc = CentralisedMpc()\n",
    "learnable_pars = LearnableParametersDict[cs.SX](\n",
    "    (\n",
    "        LearnableParameter(name, val.shape, val, sym=mpc.parameters[name])\n",
    "        for name, val in mpc.learnable_pars_init.items()\n",
    "    )\n",
    ")\n",
    "ep_len = int(100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# environment and RL training\n",
    "# instantiate power system environment\n",
    "# uses monitorepisodes to track episode performance\n",
    "env = MonitorEpisodes(TimeLimit(PowerSystem(), max_episode_steps=int(ep_len)))\n",
    "agent = Log(  # type: ignore[var-annotated]\n",
    "    RecordUpdates(\n",
    "        LoadedLstdQLearningAgentCoordinator(\n",
    "            rho=LocalMpc.rho,\n",
    "            n=n,\n",
    "            G=G,\n",
    "            Adj=Adj,\n",
    "            centralised_flag=CENTRALISED,\n",
    "            centralised_debug=True,\n",
    "            mpc_cent=mpc,\n",
    "            learnable_parameters=learnable_pars,\n",
    "            fixed_parameters=mpc.fixed_pars_init,\n",
    "            mpc_dist_list=mpc_dist_list,\n",
    "            learnable_dist_parameters_list=learnable_dist_parameters_list,\n",
    "            fixed_dist_parameters_list=fixed_dist_parameters_list,\n",
    "            discount_factor=discount_factor,\n",
    "            update_strategy=ep_len,\n",
    "            learning_rate=ExponentialScheduler(7e-7, factor=0.98),  # 5e-6\n",
    "            hessian_type=\"none\",\n",
    "            record_td_errors=True,\n",
    "            exploration=EpsilonGreedyExploration(  # None,\n",
    "                epsilon=ExponentialScheduler(0.5, factor=0.8),\n",
    "                strength=0.1 * (2 * 0.2),\n",
    "                seed=1,\n",
    "            ),\n",
    "            experience=ExperienceReplay(  # None,\n",
    "                maxlen=3 * ep_len,\n",
    "                sample_size=int(1.5 * ep_len),\n",
    "                include_latest=ep_len,\n",
    "                seed=1,\n",
    "            ),  # None,\n",
    "        )\n",
    "    ),\n",
    "    level=logging.DEBUG,\n",
    "    log_frequencies={\"on_timestep_end\": 1},\n",
    ")\n",
    "\n",
    "identifier = \"line_40_with_con_eval\"\n",
    "num_eps = 100\n",
    "if LEARN:\n",
    "    agent.train(env=env, episodes=num_eps, seed=1)\n",
    "else:\n",
    "    agent.evaluate(env=env, episodes=num_eps, seed=1)\n",
    "\n",
    "\n",
    "# Data storage and analysis\n",
    "# extract data\n",
    "if len(env.observations) > 0:\n",
    "    X = np.hstack([env.observations[i].squeeze().T for i in range(num_eps)]).T\n",
    "    U = np.hstack([env.actions[i].squeeze().T for i in range(num_eps)]).T\n",
    "    R = np.hstack([env.rewards[i].squeeze().T for i in range(num_eps)]).T\n",
    "else:\n",
    "    X = np.squeeze(env.ep_observations)\n",
    "    U = np.squeeze(env.ep_actions)\n",
    "    R = np.squeeze(env.ep_rewards)\n",
    "\n",
    "R_eps = [sum(R[ep_len * i : ep_len * (i + 1)]) for i in range(num_eps)]\n",
    "param_dict = {}\n",
    "time = np.arange(R.size)\n",
    "TD = []\n",
    "TD_eps = []\n",
    "if LEARN:\n",
    "    TD = np.squeeze(agent.td_errors) if CENTRALISED else agent.agents[0].td_errors\n",
    "    TD_eps = [sum(TD[ep_len * i : ep_len * (i + 1)]) / ep_len for i in range(num_eps)]\n",
    "    if CENTRALISED:\n",
    "        for name in mpc.to_learn:\n",
    "            param_dict[name] = np.asarray(agent.updates_history[name])\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            for name in mpc_dist_list[i].to_learn:\n",
    "                param_dict[name + \"_\" + str(i)] = np.asarray(\n",
    "                    agent.agents[i].updates_history[name]\n",
    "                )\n",
    "\n",
    "if STORE_DATA:\n",
    "    with open(\n",
    "        \"data/power_C_\"\n",
    "        + str(CENTRALISED)\n",
    "        + identifier\n",
    "        + datetime.datetime.now().strftime(\"%d%H%M%S%f\")\n",
    "        + \".pkl\",\n",
    "        \"wb\",\n",
    "    ) as file:\n",
    "        pickle.dump(X, file)\n",
    "        pickle.dump(U, file)\n",
    "        pickle.dump(R, file)\n",
    "        pickle.dump(TD, file)\n",
    "        pickle.dump(param_dict, file)\n",
    "\n",
    "if PLOT:\n",
    "    plot_power_system_data(\n",
    "        TD, R, TD_eps, R_eps, X, U, param_dict=param_dict if LEARN else None\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
