{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de76e60b-8a0d-49f8-85d4-04c9947e7f87",
   "metadata": {},
   "source": [
    "# Key Ideas of the Paper\n",
    "\n",
    "1. Description Length as a Regularization Technique:\n",
    "   * The central idea is to minimize the total description length of a neural network, which includes the weights and the error of the model on the training data. This aligns with Occam's Razor, preferring simpler models that explain the data well.\n",
    "   * Description length combines two parts:\n",
    "     1. Encoding the weights: Shorter codes for smaller weights to encourage simplicity.\n",
    "     2. Ecoding the data errors: A model with fewer errors on the training set requires less information to describe.\n",
    "    \n",
    "2. Bayesian Approach:\n",
    "   * The approach incorporates a Bayesian framework, interpreting weight regulaization as applying a prior distribution on the weights.\n",
    "   * Small weights are preferred, as they correspond to simpler models and can generalize better.\n",
    "  \n",
    "3. Practical Impact:\n",
    "   * The methodology introduces a penalty term based on the complexity of the model, which effectively prevents overfitting by discouraging overly complex solutions.\n",
    "  \n",
    "4. Connection to Modern Techniques:\n",
    "   * This work is an early precursor to later techniques such as:\n",
    "     * Weight pruning: Reudcing the size of neural networks by removing insignificant weights.\n",
    "     * Sparse coding: Encouraging sparsity in neural network representations.\n",
    "     * Variational Baysian methods: Modern Bayesian techniques used in deep learning.\n",
    "   * The ideas also relate to modern approaches like variational autoencoders (VAEs) and neural network compression techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3258e-3b99-4cbd-8538-bf8826a495ac",
   "metadata": {},
   "source": [
    "# Section 7 of the paper:\n",
    "## A coding scheme that uses a mixture of Gaussians\n",
    "\n",
    "Suppose that a sender and a receiver have already agreed on a particular mixture of Gaussians distribtion. The sender can now send a sample from the posterior Gaussian distribution of a weight using the following coding scheme: \n",
    "\n",
    "1. Randomly pick one of the Gaussians in the mixture with probability $r_i$ give by:\n",
    "   $\\displaystyle  r_i = \\frac {\\pi_i e^{-G_i}} {\\sum_j \\pi_j e^{-G_j}}$\n",
    "\n",
    "2. Communicate the choice of Gaussian to the receiver. If we use the mixing proportions as a prior for communicating the choice, the expected code cost is:\n",
    "   $\\displaystyle\\sum_{i} r_i log \\frac {1}{\\pi_i}$\n",
    "\n",
    "3. Communicate the sample value to the receiver using the chosen Gaussian. If we take into account the random bits that we get back when the receiver reconstructs the posterior distrition from which the sample was chosen, the expected cost of communicating the sample is:\n",
    "   $\\displaystyle \\sum_{i} r_i G_i$\n",
    "\n",
    "   So the expected cost of communicating both the choice of Gaussian and the sample value given that choice is:\n",
    "   $\\displaystyle \\sum_{i} r_i G_i + \\sum_{i} r_i log\\frac{1}{\\pi_i} = \\sum_{i} r_i (-log \\pi_i e^{-G_i})$\n",
    "\n",
    "4. After receiving samples from all the posterior weight distributions and also receiving the errors on the training cases with these sampled weights, the receiver can run the learning algorithm and reconstruct the posterior distributions from which the weights are sampled. This allows the receiver to reconstruct all of the $G_i$ and hence to reconstruct the random bits used to choose a Gaussian from the mixture. So the number of \"bits back\" that must be substracted from the expected cost:\n",
    "   $\\displaystyle H = \\sum_{i} r_i log \\frac{1}{r_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33baa0-7a18-455b-9663-040c96477f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
