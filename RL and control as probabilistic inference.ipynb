{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de50cc7",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "* Crucially in the framework of PGMs (probabilistic graphical models), it is sufficient to write down the model and pose the question, and the objectives for learning and inference emergy automatically. \n",
    "\n",
    "* Conventionally, decision making problems formalized as reinforcement learning or optimal control have been cast into a framework that aims to generalize probabilistic models by augmentation them with utilities or rewards, where the reward function is viewed as an extrinsic signal. In this view, determining an optimal course of action (a plan) or an optimal decision-making strategy (a policy) is a fundamentally distinct type of problem than probabilistic inference, although the underlying dynamical system might still be described by a probabilistic graphical model. \n",
    "\n",
    "* Formalizing decision making as inference in probabilistic graphical models can in principle allow us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. \n",
    "\n",
    "* All these methods (maximum entropy reinforcement learning) involve formulating control or reinforcement learning as a PGM, either explicitly or implictly, and then deploying learning and inference methods from the PGM literature to solve the resulting inference and leraning problems. \n",
    "\n",
    "* Formulating RL and decision making as inference provides a number of other appealing tools: a natural exploration strategy based on entropy maximization, effective tools for inverse RL, and the ability to deploy powerful approximate inference algorithms to solve RL problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2aacd0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
