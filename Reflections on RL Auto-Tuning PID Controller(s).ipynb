{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "558fff62-bec6-4bdc-a88e-103dc958404b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# High-level Outline of Wrapping a RL algorithm around a PID Tuning Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e28b1-f342-40b4-9d7b-767c7b7b586c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "1. **Define the Control Problem and Environment.**\n",
    "   * System Dynamics: for example, a second-order mass-spring-damper problem\n",
    "   * Controller: A PID controller parameterized by\n",
    "     $$C(s) = K_p + \\frac{K_i}{s} + K_d s$$\n",
    "   * Reference Signal: Set point tracking problem/ A trajectory we want the system to follow\n",
    "   * State Representations: IMPORTANT!\n",
    "     In an RL setting, we need to decide what the state vector (input to the RL agent) will be. Possible choices:\n",
    "     * The current tracking error $e(t) = r(t) - y(t)$\n",
    "     * The derivative of the error $\\dot{e}(t)$\n",
    "     * Possibly the integral of error, or the system's internal state if accessible\n",
    "   * Actions: In RL-for-control problems, the agent's action is to update the controller parameters. Therefore, it can be set to directly output $\\Delta K_p, \\Delta K_i, \\Delta K_d$ or directly pick the value for bandwidth $\\omega_{\\text CL}$.\n",
    "\n",
    "2. **Choose the Reward Function.**\n",
    "   * A typical reward function for controller tuning penalizes tracking error and actuator effort.\n",
    "     $$r = -\\left( \\alpha |e(t) + \\beta u(t)^2 \\right)$$\n",
    "     where $u(t)$ is the control signal (to limit large control efforts), and $\\alpha, \\beta$ are weighting factors.\n",
    "   * Alternatively, reward can be defined based on IAE, ISE, settling time, overshoot, etc (or the combination of all of them, connected with different weighting factors). The key is to design reward function to reflect the \"good control performance\" means for the application. \n",
    "\n",
    "3. **Implementation Flow (Online).**\n",
    "   * Based Nathan's work, RL should be implemented online $\\rightarrow$ RL agent is learning in real time on physical or simulated system. Here's the flow:\n",
    "     1. Initialize PID gains (+safty considerations/constraints); Initialize RL internal parameters (e.g., NN weights in a policy gradient method; Q-table; etc)\n",
    "     2. Observe the state: at each control cycle, measure or estimate the state $s_t$ $\\rightarrow$ this is difficult because full observation of the environment is not always available. \n",
    "     3. Action selection:\n",
    "        RL agent chooses an action $a_t$:\n",
    "        $$a_t = \\left[ \\Delta K_p, \\Delta K_i, \\Delta K_d \\right]$$ or $$a_t = \\Delta \\omega_{\\text CL}$$\n",
    "        Then update the current gains accordingly.\n",
    "     4. Apply controller: use the updated PID gains to compute $u(t)$ for the system; Let the system run for one control inverval (time horizon) under the NEW PID GAINs\n",
    "     5. Reward Computation: After this step/horizon, compute the immediate reward $r_t$ based on the observed tracking performance (reward function)\n",
    "     6. Agent Update: Use $[s_t, a_t, r_t, s_{t+1}]$ to update the RL agent's (a) policy in policy gradient; (b) value function in Q-learning or actor-critic networks in real time.\n",
    "     7. Repeat: Move to the next time step; RL should discover how to adjust PID gains (or bandwidth) to maximize reward (gradually) $\\rightarrow$ HOW TO ENSURE FAST CONVERGENCE?\n",
    "\n",
    "4. **Choice of Algorithms.**\n",
    "   * MODEL-FREE METHODS (Q‐Learning, DDPG, PPO, SAC):\n",
    "     Typically represent the policy by actor network that inputs the current system state and outputs the increment in PID gains.Advantage: don’t need an explicit model of your system. Disadvantage: potentially slower learning and need for careful hyperparameter tuning, exploration strategies, etc.\n",
    "   * MODEL-BASED RL: not common for PID tuning tasks -- HOW ABOUT A PARTIAL MODEL? CAN RL LEARN A MODEL PURELY FROM DATA (CAUSAL INFERENCE)\n",
    "   * Adaptive/Hybrid approaches: Tune a standard PID structure using approximate methods?\n",
    "\n",
    "5. **Considerations of Implementation in Practical.**\n",
    "   * Safety: how to constrain RL exploration (boundary settings)\n",
    "   * Rate of Updates: In real settings (especially process industries), can't/don't need to adjust PID gains on every single time step. Solution: run the system for a short period (enough time to observe some transients) $\\rightarrow$ compute a performance metric $\\rightarrow$ only update the gains\n",
    "   * Reward Shaping: Don't rely on single episode reward because it slows learning. Need to design instantaneous reward to help RL agent converge faster\n",
    "   * Discrete vs. Continuous action space: for small step in $\\omega_{\\text CL}$ or in $\\Delta K_p$, a continuous action RL algorithm is preferred. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aedc11b-347a-43eb-9d06-3faed090f8b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e053a3c-9df1-4e0b-8bfa-90b9d50c0afd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model-free RL, continuous action\n",
    "\n",
    "# 1) Initialize environment\n",
    "env = FirstOrderEnv()  \n",
    "\n",
    "# 2) Initialize RL agent .\n",
    "agent = RLAgent(policy_network, ...)\n",
    "\n",
    "# 3) PID parameters or omega_CL start with a safe initial guess\n",
    "Kp, Ki, Kd = Kp0, Ki0, Kd0\n",
    "env.set_pid_gains(Kp, Ki, Kd)\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    state = env.reset()  # e.g. reset error, integrators, etc.\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Agent selects an action based on current state\n",
    "        action = agent.select_action(state)  \n",
    "        # E.g. action = [dKp, dKi, dKd]\n",
    "        \n",
    "        # Update PID gains\n",
    "        Kp += action[0]\n",
    "        Ki += action[1]\n",
    "        Kd += action[2]\n",
    "        env.set_pid_gains(Kp, Ki, Kd)\n",
    "        \n",
    "        # Step the environment for one (or several) time steps\n",
    "        next_state, reward, done, info = env.step()\n",
    "        \n",
    "        # Agent learns from this transition\n",
    "        agent.update(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    \n",
    "    print(f\"Episode {episode}, total_reward: {episode_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4cd231-a6bf-4df1-bf29-71960b8d95f9",
   "metadata": {},
   "source": [
    "# Potential Modifications and Extensions on the Existing Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f71282-4d4e-4f73-a71a-7fe1a70222fd",
   "metadata": {},
   "source": [
    "1. Multi-Loop and MIMO Extensions\n",
    "   * Multi-agent RL: Assign one agent per PID loop, with either local or shared rewards (depending on coupling)\n",
    "   * Centralized Critic, Decentralized Actors: A central critic can observe global state for better training feedback, while each loop's actor use local info at runtime.\n",
    "\n",
    "2. Safe or Constrained RL\n",
    "   * Action Bounding: Keep PID parameters within known stability bounds\n",
    "   * Lyapunov-based Critics or Barrier Functions: incorporate formal constraints so that actions violating stability or safety constraints are automatically corrected.\n",
    "\n",
    "3. Robust or Adaptive RL for Nonstationary Environments: because industrial processes drift over time\n",
    "   * Online Fine-Tuning: Keep updating the actor/critic with a small learning rate as the environment changes\n",
    "   * Domain Randomization in simulation: produce better generalization abilities by training across a range of process parameters or disturbances.\n",
    "\n",
    "4. Hybrid Approaches: Model-based (or partially model-based) + RL\n",
    "   * Use an MPC-based safe supervisor to override dangerous actions\n",
    "   * Model-Reference RL: leverage a known or approximated plant model to shape the reward or guide exploration\n",
    "\n",
    "5. Sophisticated Reward Function (unique design for different needs)\n",
    "   * Multi-objective Reward: Weighted sum approach for different KPIs\n",
    "   * Sparse or Event-Driven Rewards: If real-time feedback is not always available or if certain events are very costly, modify the reward to reflect these priorities.\n",
    "\n",
    "6. Extended Testing and Benchmarking\n",
    "   * Benchmark Processes: test on well-estabilished control problems\n",
    "   * Compare with Adaptive Control\n",
    "\n",
    "7. Detailed Stability & Performance Analysis\n",
    "   * Sensitivity Analysis\n",
    "   * Convergence Guarantees (math proofs)\n",
    "\n",
    "8. Other thoughts:\n",
    "   * Train offline first, then gradually adapt online\n",
    "   * Let RL controller run in parallel with existing PID, compare performance, and only swtich over when proven superior\n",
    "   * Transfer learning (could be from one PID controller to another PID controller, or could be from a plant to another plant)\n",
    "   * model interpretability: sequential decision making? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c82148-af32-48c1-9ed2-20e4afd0583f",
   "metadata": {},
   "source": [
    "# Wild Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd10f8f2-9dcf-44f4-92da-2c03c972e62f",
   "metadata": {},
   "source": [
    "Research Topic: Multi-Agent Reinforcement Learning for Coordinated Tuning of Multiple PID Controllers in Large-Scale or MIMO Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d6d38b-724d-49d1-bcfd-284fbea110e7",
   "metadata": {},
   "source": [
    "Context and motivation:\n",
    "\n",
    "* In many industrial processes, you often have multiple control loops running in parallel. Each loop is managed by a PID controller.\n",
    "* These loops can be highly coupled, meaning the tuning of one PID loop may affect the performance of others due to process interactions, shared resources, or physical couplings.\n",
    "* Traditional tuning methods (like Ziegler-Nichols) do not account for these interactions, which can lead to suboptimal or unstable performance at the plant level. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48829e02-c61e-4b07-b81d-adbd2b982e32",
   "metadata": {},
   "source": [
    "Core Idea:\n",
    "\n",
    "* Use a multi-agent reinforcement learning (MARL) approach in which each PID loop is tuned by a separate RL agent in real time (or near-real time).\n",
    "* The agents must coordinate with each other through (makes me think of social RL) a shared reward or communication mechanism to ensure that the overall plant performance is optimized rather than each loop optimizing its own performance in isolation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c42f94-fa4c-4f80-b15a-d81c780b7f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
