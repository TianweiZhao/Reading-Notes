{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13170ac",
   "metadata": {},
   "source": [
    "# Question: RL for PID Auto-Tuning in Multi-Loop Systems (Offline)\n",
    "\n",
    "## Goal:\n",
    "\n",
    "Use **offline RL** to **learn a policy** that:\n",
    "* Takes system context (plant dynamics, recent states, error history, etc. )\n",
    "* Outputs a full set of optimal PID parameters $K = [K_p^1, K_i^1, K_d^1, ..., K_p^N, K_i^N, K_d^N]$\n",
    "\n",
    "## Challenges:\n",
    "\n",
    "* High-dimensional action space for large N loops\n",
    "* Inter-loop coupling -> optimal gains depend on other loops\n",
    "* Mutli-modal solutions: multiple gain sets may perform similarly well\n",
    "* Offline-only data -> can't interact online to improve\n",
    "\n",
    "## Strengths\n",
    "\n",
    "1. Multimodal action modeling:\n",
    "    - many different gain sets can yield good control; diffusion handles this better than Gaussian policies\n",
    "2. Smooth action trajectories:\n",
    "    - outputting sequences of gains (e.g., adapting gains over a window) benefits from smooth, coherent generation\n",
    "3. Flexible conditioning:\n",
    "    - Can condition on error curves, setpoint changes, prior performance - allowing fine-grained gain generation\n",
    "4. Action space scalability: \n",
    "    - Diffusion models scale well to large output spaces (hundreds of dimensions), which suits multi-loop systems. \n",
    "5. Offline training performance:\n",
    "    - diffusion policies outperform traditional BC in many offline scenarios by better modeling expert behavior. \n",
    "\n",
    "\n",
    "## Challenges: \n",
    "\n",
    "1. Inference speed:\n",
    "    - If gains must be tuned frequently (e.g., per setpoint change), slow sampling may hinder real-time applicability\n",
    "2. Interpretability:\n",
    "    - we care why a certain gain set was chosen - diffusion policies are harder to interpret unless distilled\n",
    "3. Reward alignment:\n",
    "    - Training depends on how clearly performance (e.g., overshoot, settling time) is reflected in data\n",
    "4. Sparse or biased datasets:\n",
    "    - If most offline data uses \"safe but suboptimal\" gains, the learned model may not generalize to better ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ccdb9",
   "metadata": {},
   "source": [
    "# Design considerations: Diffusion-Based PID Auto-Tuner\n",
    "\n",
    "**Inputs**:\n",
    "\n",
    "* Current plant state and recent history $x_{t-w:t}$\n",
    "* Setpoint $r_t$ or setpoint change trajectory\n",
    "* Past controller performance metrics (e.g., IAE, overshoot)\n",
    "\n",
    "\n",
    "**Output**:\n",
    "* Full vector of PID gains for all loops\n",
    "\n",
    "**Training Flow**:\n",
    "1. Collect offline dataset of:\n",
    "    $$ (context, K_{used}, performance) $$\n",
    "2. Corrupt PID gains with noise over T steps\n",
    "3. Train diffusion model to denoise PID gains, conditioned on context\n",
    "4. At inference, sample noise, denoise it based on new plant context -> get tuned PID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e16c1",
   "metadata": {},
   "source": [
    "# Compared with other RL algorithms:\n",
    "\n",
    "1. SAC / TD3 (continuous RL): can learn good gains, but struggles with mutlimodality and large joint action spaces\n",
    "2. Decision Transformer (DT): can learn to condition on performance goals and trajectories; still needs large datasets and reward shaping\n",
    "3. Diffusion policy: Excels in multimodal, high-dimensional action spaces - a very promising fit for offline PID gain learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a30cc58",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5944c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import gymnasium as gym\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the CSTR environment\n",
    "# Make sure CSTR_model_plus.py is in the same directory\n",
    "from CSTR_model_plus import CSTRRLEnv\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#######################\n",
    "# Data Generation\n",
    "#######################\n",
    "\n",
    "def generate_pid_dataset(num_episodes=1000, time_steps=100, save_path='pid_dataset.pt'):\n",
    "    \"\"\"Generate a dataset of PID gain trajectories and their performance on the CSTR system.\n",
    "    \n",
    "    Args:\n",
    "        num_episodes: Number of episodes (different PID settings) to generate\n",
    "        time_steps: Number of time steps per episode\n",
    "        save_path: Path to save the dataset\n",
    "        \n",
    "    Returns:\n",
    "        dataset: Dictionary containing the dataset\n",
    "    \"\"\"\n",
    "    # Initialize environment\n",
    "    env = CSTRRLEnv(simulation_steps=time_steps, \n",
    "                    uncertainty_level=0.1,\n",
    "                    noise_level=0.02,\n",
    "                    actuator_delay_steps=1,\n",
    "                    transport_delay_steps=2,\n",
    "                    enable_disturbances=True)\n",
    "    \n",
    "    # Lower and upper bounds for PID gains\n",
    "    pid_lower = np.array([-5, 0, 0.02, 0, 0, 0.01])\n",
    "    pid_upper = np.array([25, 20, 10, 1, 2, 1])\n",
    "    \n",
    "    dataset = {\n",
    "        \"state_traj\": [],           # State trajectory\n",
    "        \"error_traj\": [],           # Error trajectory  \n",
    "        \"setpoint_traj\": [],        # Setpoint trajectory\n",
    "        \"pid_gains\": [],            # PID gains (actions)\n",
    "        \"performance\": [],          # Performance metrics\n",
    "        \"metadata\": {               # Metadata\n",
    "            \"pid_lower\": pid_lower.tolist(),\n",
    "            \"pid_upper\": pid_upper.tolist(),\n",
    "            \"time_steps\": time_steps,\n",
    "            \"num_episodes\": num_episodes,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Generating {num_episodes} episodes...\")\n",
    "    \n",
    "    for episode in tqdm.tqdm(range(num_episodes)):\n",
    "        # Reset the environment with different setpoints for better exploration\n",
    "        # Create varying setpoints for Cb and V\n",
    "        setpoints_Cb = [0.5 + 0.5 * np.random.rand(), \n",
    "                        0.5 + 0.5 * np.random.rand()]\n",
    "        setpoints_V = [99.0 + 2.0 * np.random.rand(), \n",
    "                      99.0 + 2.0 * np.random.rand()]\n",
    "        setpoint_durations = [time_steps // 2, time_steps // 2 + time_steps % 2]\n",
    "        \n",
    "        options = {\n",
    "            'setpoints_Cb': setpoints_Cb,\n",
    "            'setpoints_V': setpoints_V,\n",
    "            'setpoint_durations': setpoint_durations\n",
    "        }\n",
    "        \n",
    "        obs, _ = env.reset(seed=episode, options=options)\n",
    "        \n",
    "        # Sample random PID gains (normalized between -1 and 1 for the action space)\n",
    "        # We'll have a mix of:\n",
    "        # 1. Random gains (50%)\n",
    "        # 2. Expert-tuned gains with noise (50%)\n",
    "        \n",
    "        if np.random.rand() < 0.5:\n",
    "            # Random gains\n",
    "            action = np.random.uniform(-1, 1, size=6)\n",
    "        else:\n",
    "            # Start with some good PID values and add noise\n",
    "            # These values are heuristics that work reasonably well for this system\n",
    "            good_pid_normalized = np.array([0.5, 0.3, 0.2, 0.4, 0.3, 0.2])  # Normalized good PID values\n",
    "            # Convert from normalized to actual\n",
    "            good_pid = ((good_pid_normalized + 1) / 2) * (pid_upper - pid_lower) + pid_lower\n",
    "            # Add noise\n",
    "            good_pid *= (1 + 0.3 * np.random.randn(6))\n",
    "            # Convert back to normalized\n",
    "            action = 2 * (good_pid - pid_lower) / (pid_upper - pid_lower) - 1\n",
    "            # Clip to valid range\n",
    "            action = np.clip(action, -1, 1)\n",
    "        \n",
    "        # Storage for trajectories\n",
    "        state_traj = []\n",
    "        error_traj = []\n",
    "        setpoint_traj = []\n",
    "        \n",
    "        # Run the episode with fixed PID gains\n",
    "        done = False\n",
    "        step = 0\n",
    "        cumulative_reward = 0\n",
    "        \n",
    "        # Statistics for performance metrics\n",
    "        abs_Cb_errors = []\n",
    "        abs_V_errors = []\n",
    "        Cb_setpoint_changes = []\n",
    "        V_setpoint_changes = []\n",
    "        controller_efforts = []\n",
    "        \n",
    "        while not done:\n",
    "            # Store trajectory information\n",
    "            # Extract values from observation\n",
    "            current_Cb = obs[0]  # Current Cb is at index 0\n",
    "            current_T = obs[1]   # Current T is at index 1\n",
    "            current_V = obs[2]   # Current V is at index 2\n",
    "            \n",
    "            current_setpoint_Cb = obs[9]   # Current setpoint Cb is at index 9\n",
    "            current_setpoint_V = obs[10]   # Current setpoint V is at index 10\n",
    "            \n",
    "            # Calculate errors\n",
    "            error_Cb = current_setpoint_Cb - current_Cb\n",
    "            error_V = current_setpoint_V - current_V\n",
    "            \n",
    "            # Store trajectories\n",
    "            state_traj.append([current_Cb, current_T, current_V])\n",
    "            error_traj.append([error_Cb, error_V])\n",
    "            setpoint_traj.append([current_setpoint_Cb, current_setpoint_V])\n",
    "            \n",
    "            # Take action (use the same PID gains throughout the episode)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            # Track performance metrics\n",
    "            abs_Cb_errors.append(abs(error_Cb))\n",
    "            abs_V_errors.append(abs(error_V))\n",
    "            \n",
    "            # Track control effort (changes in control signals)\n",
    "            if step > 0:\n",
    "                Tc_change = abs(info[\"control_action\"][0] - prev_control_action[0])\n",
    "                Fin_change = abs(info[\"control_action\"][1] - prev_control_action[1])\n",
    "                controller_efforts.append([Tc_change, Fin_change])\n",
    "            \n",
    "            prev_control_action = info[\"control_action\"]\n",
    "            \n",
    "            # Track setpoint changes\n",
    "            if step > 0:\n",
    "                Cb_setpoint_change = abs(current_setpoint_Cb - prev_setpoint_Cb)\n",
    "                V_setpoint_change = abs(current_setpoint_V - prev_setpoint_V)\n",
    "                if Cb_setpoint_change > 0:\n",
    "                    Cb_setpoint_changes.append(step)\n",
    "                if V_setpoint_change > 0:\n",
    "                    V_setpoint_changes.append(step)\n",
    "            \n",
    "            prev_setpoint_Cb = current_setpoint_Cb\n",
    "            prev_setpoint_V = current_setpoint_V\n",
    "            \n",
    "            # Update for next iteration\n",
    "            obs = next_obs\n",
    "            cumulative_reward += reward\n",
    "            step += 1\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        iae_Cb = np.sum(abs_Cb_errors)  # Integral Absolute Error for Cb\n",
    "        iae_V = np.sum(abs_V_errors)    # Integral Absolute Error for V\n",
    "        \n",
    "        # Calculate rise time and settling time for each setpoint change\n",
    "        rise_times = []\n",
    "        settling_times = []\n",
    "        \n",
    "        # Calculate mean control effort\n",
    "        mean_control_effort_Tc = np.mean([e[0] for e in controller_efforts]) if controller_efforts else 0\n",
    "        mean_control_effort_Fin = np.mean([e[1] for e in controller_efforts]) if controller_efforts else 0\n",
    "        \n",
    "        # Overall performance score (lower is better)\n",
    "        performance_score = iae_Cb * 0.7 + iae_V * 0.2 + mean_control_effort_Tc * 0.05 + mean_control_effort_Fin * 0.05\n",
    "        \n",
    "        # Calculate overshoot\n",
    "        overshoots = []\n",
    "        \n",
    "        # Store all the information in the dataset\n",
    "        dataset[\"state_traj\"].append(np.array(state_traj))\n",
    "        dataset[\"error_traj\"].append(np.array(error_traj))\n",
    "        dataset[\"setpoint_traj\"].append(np.array(setpoint_traj))\n",
    "        dataset[\"pid_gains\"].append(action)  # Store the normalized action\n",
    "        \n",
    "        # Store performance metrics\n",
    "        dataset[\"performance\"].append({\n",
    "            \"iae_Cb\": iae_Cb,\n",
    "            \"iae_V\": iae_V,\n",
    "            \"mean_control_effort_Tc\": mean_control_effort_Tc,\n",
    "            \"mean_control_effort_Fin\": mean_control_effort_Fin,\n",
    "            \"performance_score\": performance_score,\n",
    "            \"cumulative_reward\": cumulative_reward\n",
    "        })\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    dataset[\"pid_gains\"] = np.array(dataset[\"pid_gains\"])\n",
    "    \n",
    "    # Save the dataset\n",
    "    torch.save(dataset, save_path)\n",
    "    print(f\"Dataset saved to {save_path}\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def filter_dataset(dataset, performance_threshold=0.7):\n",
    "    \"\"\"Filter the dataset based on performance metrics to keep only good examples.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Original dataset\n",
    "        performance_threshold: Threshold percentile for filtering (0.7 means keep the top 30%)\n",
    "        \n",
    "    Returns:\n",
    "        filtered_dataset: Filtered dataset\n",
    "    \"\"\"\n",
    "    # Get performance scores\n",
    "    performance_scores = np.array([p[\"performance_score\"] for p in dataset[\"performance\"]])\n",
    "    \n",
    "    # Lower score is better, so we want to keep the bottom percentage\n",
    "    threshold = np.percentile(performance_scores, performance_threshold * 100)\n",
    "    \n",
    "    # Find indices of episodes to keep\n",
    "    keep_indices = np.where(performance_scores <= threshold)[0]\n",
    "    \n",
    "    # Create filtered dataset\n",
    "    filtered_dataset = {\n",
    "        \"state_traj\": [dataset[\"state_traj\"][i] for i in keep_indices],\n",
    "        \"error_traj\": [dataset[\"error_traj\"][i] for i in keep_indices],\n",
    "        \"setpoint_traj\": [dataset[\"setpoint_traj\"][i] for i in keep_indices],\n",
    "        \"pid_gains\": dataset[\"pid_gains\"][keep_indices],\n",
    "        \"performance\": [dataset[\"performance\"][i] for i in keep_indices],\n",
    "        \"metadata\": dataset[\"metadata\"]\n",
    "    }\n",
    "    \n",
    "    print(f\"Filtered dataset: kept {len(keep_indices)} out of {len(dataset['pid_gains'])} episodes ({performance_threshold * 100:.1f}% best performers)\")\n",
    "    \n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "class PIDDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for PID controller tuning.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, window_size=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: Dictionary containing the dataset\n",
    "            window_size: Size of the sliding window for state/error/setpoint trajectories\n",
    "        \"\"\"\n",
    "        self.state_traj = dataset[\"state_traj\"]\n",
    "        self.error_traj = dataset[\"error_traj\"]\n",
    "        self.setpoint_traj = dataset[\"setpoint_traj\"]\n",
    "        self.pid_gains = torch.tensor(dataset[\"pid_gains\"], dtype=torch.float32)\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Metadata\n",
    "        self.pid_lower = torch.tensor(dataset[\"metadata\"][\"pid_lower\"], dtype=torch.float32)\n",
    "        self.pid_upper = torch.tensor(dataset[\"metadata\"][\"pid_upper\"], dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pid_gains)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get trajectories\n",
    "        state = self.state_traj[idx]\n",
    "        error = self.error_traj[idx]\n",
    "        setpoint = self.setpoint_traj[idx]\n",
    "        \n",
    "        # Get random window if trajectory is longer than window_size\n",
    "        traj_len = len(state)\n",
    "        if traj_len > self.window_size:\n",
    "            start_idx = np.random.randint(0, traj_len - self.window_size)\n",
    "            end_idx = start_idx + self.window_size\n",
    "            \n",
    "            state_window = state[start_idx:end_idx]\n",
    "            error_window = error[start_idx:end_idx]\n",
    "            setpoint_window = setpoint[start_idx:end_idx]\n",
    "        else:\n",
    "            # Pad with zeros if trajectory is shorter than window_size\n",
    "            state_window = np.pad(state, ((0, self.window_size - traj_len), (0, 0)), mode='constant')\n",
    "            error_window = np.pad(error, ((0, self.window_size - traj_len), (0, 0)), mode='constant')\n",
    "            setpoint_window = np.pad(setpoint, ((0, self.window_size - traj_len), (0, 0)), mode='constant')\n",
    "        \n",
    "        # Convert to tensors\n",
    "        state_tensor = torch.tensor(state_window, dtype=torch.float32)\n",
    "        error_tensor = torch.tensor(error_window, dtype=torch.float32)\n",
    "        setpoint_tensor = torch.tensor(setpoint_window, dtype=torch.float32)\n",
    "        pid_gains = self.pid_gains[idx]\n",
    "        \n",
    "        return {\n",
    "            \"state_traj\": state_tensor,\n",
    "            \"error_traj\": error_tensor,\n",
    "            \"setpoint_traj\": setpoint_tensor,\n",
    "            \"pid_gains\": pid_gains\n",
    "        }\n",
    "\n",
    "\n",
    "#######################\n",
    "# Model Architecture\n",
    "#######################\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal time embedding for diffusion timesteps.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = t[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((torch.sin(embeddings), torch.cos(embeddings)), dim=-1)\n",
    "        \n",
    "        # Zero-pad if dim is odd\n",
    "        if self.dim % 2 == 1:\n",
    "            embeddings = F.pad(embeddings, (0, 1, 0, 0))\n",
    "            \n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class TrajectoryEncoder(nn.Module):\n",
    "    \"\"\"Encoder for trajectory data to provide context for the diffusion model.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim=3, error_dim=2, setpoint_dim=2, hidden_dim=128, context_dim=128, window_size=10):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Input dimensions for combined trajectory features\n",
    "        input_dim = state_dim + error_dim + setpoint_dim\n",
    "        \n",
    "        # LSTM to process the sequence\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Projection to context embedding\n",
    "        self.context_projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, context_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state_traj, error_traj, setpoint_traj):\n",
    "        batch_size = state_traj.shape[0]\n",
    "        \n",
    "        # Concatenate all features along feature dimension\n",
    "        # Shape: [batch_size, window_size, state_dim + error_dim + setpoint_dim]\n",
    "        combined_input = torch.cat([state_traj, error_traj, setpoint_traj], dim=2)\n",
    "        \n",
    "        # Process sequence with LSTM\n",
    "        lstm_out, (hidden, _) = self.lstm(combined_input)\n",
    "        \n",
    "        # Use the final hidden state\n",
    "        # Shape: [batch_size, hidden_dim]\n",
    "        final_hidden = hidden[-1]\n",
    "        \n",
    "        # Project to context embedding\n",
    "        # Shape: [batch_size, context_dim]\n",
    "        context = self.context_projection(final_hidden)\n",
    "        \n",
    "        return context\n",
    "\n",
    "\n",
    "class DiffusionBlock(nn.Module):\n",
    "    \"\"\"Basic building block for the diffusion model.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_norm(x + self.block(x))\n",
    "\n",
    "\n",
    "class DiffusionModel(nn.Module):\n",
    "    \"\"\"Diffusion model for predicting PID gains.\"\"\"\n",
    "    \n",
    "    def __init__(self, pid_dim=6, time_dim=128, context_dim=128, hidden_dim=256, depth=6):\n",
    "        super().__init__()\n",
    "        self.pid_dim = pid_dim\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embedding = TimeEmbedding(time_dim)\n",
    "        \n",
    "        # Initial projection\n",
    "        self.input_projection = nn.Linear(pid_dim, hidden_dim)\n",
    "        \n",
    "        # Combined embedding of time and context\n",
    "        self.combined_projection = nn.Sequential(\n",
    "            nn.Linear(time_dim + context_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Main diffusion blocks\n",
    "        self.diffusion_blocks = nn.ModuleList([\n",
    "            DiffusionBlock(hidden_dim) for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Final output layer\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, pid_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t, context):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Noisy PID gains [batch_size, pid_dim]\n",
    "            t: Diffusion timesteps [batch_size]\n",
    "            context: Context embeddings [batch_size, context_dim]\n",
    "        \"\"\"\n",
    "        # Time embedding\n",
    "        t_emb = self.time_embedding(t)\n",
    "        \n",
    "        # Combine time and context\n",
    "        cond_emb = torch.cat([t_emb, context], dim=1)\n",
    "        cond = self.combined_projection(cond_emb)\n",
    "        \n",
    "        # Initial projection of noisy PID gains\n",
    "        h = self.input_projection(x)\n",
    "        \n",
    "        # Apply diffusion blocks with conditioning\n",
    "        for block in self.diffusion_blocks:\n",
    "            h = h + cond  # Add the conditioning at each step\n",
    "            h = block(h)\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.output_projection(h)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "#######################\n",
    "# Diffusion Process\n",
    "#######################\n",
    "\n",
    "def linear_beta_schedule(timesteps, beta_start=1e-4, beta_end=0.02):\n",
    "    \"\"\"Linear schedule for variance of the forward process.\"\"\"\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "\n",
    "class DiffusionTrainer:\n",
    "    \"\"\"Trainer for the diffusion model.\"\"\"\n",
    "    \n",
    "    def __init__(self, timesteps=1000):\n",
    "        # Set up the noise schedule\n",
    "        self.timesteps = timesteps\n",
    "        self.betas = linear_beta_schedule(timesteps)\n",
    "        \n",
    "        # Pre-compute values for sampling and training\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        # Calculations for diffusion q(x_t | x_{t-1})\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
    "        \n",
    "        # Calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = self.betas * (1. - self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
    "        \n",
    "    def q_sample(self, x_0, t, noise=None):\n",
    "        \"\"\"Forward diffusion process: q(x_t | x_0).\"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "            \n",
    "        # Get the scheduled values for the given timestep\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t]\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t]\n",
    "        \n",
    "        # Reshape for broadcasting\n",
    "        sqrt_alphas_cumprod_t = sqrt_alphas_cumprod_t.view(-1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod_t.view(-1, 1)\n",
    "        \n",
    "        # Forward process\n",
    "        return sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "    \n",
    "    def p_sample(self, model, x_t, t, context):\n",
    "        \"\"\"Single step of reverse diffusion process: p(x_{t-1} | x_t).\"\"\"\n",
    "        # Get model prediction (predicted noise)\n",
    "        pred_noise = model(x_t, t, context)\n",
    "        \n",
    "        # Get alpha values for timestep t\n",
    "        alpha = self.alphas[t]\n",
    "        alpha_cumprod = self.alphas_cumprod[t]\n",
    "        alpha_cumprod_prev = self.alphas_cumprod_prev[t]\n",
    "        beta = self.betas[t]\n",
    "        \n",
    "        # Reshape for broadcasting\n",
    "        alpha_view = alpha.view(-1, 1)\n",
    "        beta_view = beta.view(-1, 1)\n",
    "        \n",
    "        # Get posterior mean\n",
    "        pred_x0 = (x_t - (1 - alpha_view).sqrt() * pred_noise) / alpha_view.sqrt()\n",
    "        posterior_mean = (\n",
    "            (alpha_cumprod_prev.sqrt() / (1. - alpha_cumprod).sqrt()) * beta_view * pred_x0 +\n",
    "            ((1. - alpha_cumprod_prev).sqrt() / (1. - alpha_cumprod).sqrt()) * (1. - beta_view) * x_t\n",
    "        )\n",
    "        \n",
    "        # Get posterior variance\n",
    "        posterior_variance = beta_view * (1. - alpha_cumprod_prev) / (1. - alpha_cumprod)\n",
    "        posterior_log_variance = torch.log(posterior_variance)\n",
    "        \n",
    "        # Add noise scaled by the variance\n",
    "        noise = torch.randn_like(x_t) if t[0] > 0 else torch.zeros_like(x_t)\n",
    "        \n",
    "        # Sample from the posterior\n",
    "        x_t_prev = posterior_mean + torch.exp(0.5 * posterior_log_variance) * noise\n",
    "        \n",
    "        return x_t_prev\n",
    "    \n",
    "    def p_sample_loop(self, model, context, shape, device, verbose=True):\n",
    "        \"\"\"Full reverse diffusion from noise to data.\"\"\"\n",
    "        # Start from pure noise\n",
    "        img = torch.randn(shape).to(device)\n",
    "        \n",
    "        # Iteratively denoise\n",
    "        iterator = range(self.timesteps - 1, -1, -1)\n",
    "        if verbose:\n",
    "            iterator = tqdm.tqdm(iterator, desc=\"Sampling\")\n",
    "            \n",
    "        for i in iterator:\n",
    "            # Same timestep for entire batch\n",
    "            t = torch.full((shape[0],), i, device=device, dtype=torch.long)\n",
    "            \n",
    "            # Apply single denoising step\n",
    "            with torch.no_grad():\n",
    "                img = self.p_sample(model, img, t, context)\n",
    "                \n",
    "        return img\n",
    "    \n",
    "    def train_step(self, model, optimizer, batch, device):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        # Extract data\n",
    "        state_traj = batch[\"state_traj\"].to(device)\n",
    "        error_traj = batch[\"error_traj\"].to(device)\n",
    "        setpoint_traj = batch[\"setpoint_traj\"].to(device)\n",
    "        pid_gains = batch[\"pid_gains\"].to(device)\n",
    "        \n",
    "        # Get context\n",
    "        context = model.trajectory_encoder(state_traj, error_traj, setpoint_traj)\n",
    "        \n",
    "        # Sample timestep\n",
    "        batch_size = pid_gains.shape[0]\n",
    "        t = torch.randint(0, self.timesteps, (batch_size,), device=device).long()\n",
    "        \n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(pid_gains)\n",
    "        \n",
    "        # Forward diffusion\n",
    "        x_t = self.q_sample(pid_gains, t, noise)\n",
    "        \n",
    "        # Predict noise\n",
    "        pred_noise = model(x_t, t, context)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "#######################\n",
    "# Complete Diffusion Policy\n",
    "#######################\n",
    "\n",
    "class DiffusionPolicy(nn.Module):\n",
    "    \"\"\"Complete diffusion policy for PID controller tuning.\"\"\"\n",
    "    \n",
    "    def __init__(self, pid_dim=6, state_dim=3, error_dim=2, setpoint_dim=2, \n",
    "                 time_dim=128, context_dim=128, hidden_dim=256, \n",
    "                 depth=6, window_size=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Trajectory encoder\n",
    "        self.trajectory_encoder = TrajectoryEncoder(\n",
    "            state_dim=state_dim,\n",
    "            error_dim=error_dim,\n",
    "            setpoint_dim=setpoint_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            context_dim=context_dim,\n",
    "            window_size=window_size\n",
    "        )\n",
    "        \n",
    "        # Diffusion core model\n",
    "        self.diffusion_core = DiffusionModel(\n",
    "            pid_dim=pid_dim,\n",
    "            time_dim=time_dim,\n",
    "            context_dim=context_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            depth=depth\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t, context):\n",
    "        \"\"\"Forward pass for the diffusion model.\"\"\"\n",
    "        return self.diffusion_core(x, t, context)\n",
    "\n",
    "\n",
    "#######################\n",
    "# Training Function\n",
    "#######################\n",
    "\n",
    "def train_diffusion_policy(dataset, num_epochs=50, batch_size=32, lr=3e-4, \n",
    "                           timesteps=1000, save_path='diffusion_policy.pt',\n",
    "                           window_size=10):\n",
    "    \"\"\"Train the diffusion policy on the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dictionary containing the dataset\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        lr: Learning rate\n",
    "        timesteps: Number of diffusion timesteps\n",
    "        save_path: Path to save the model\n",
    "        window_size: Size of the trajectory window\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        trainer: Diffusion trainer\n",
    "        train_losses: List of training losses\n",
    "    \"\"\"\n",
    "    # Create dataset and dataloader\n",
    "    train_dataset = PIDDataset(dataset, window_size=window_size)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Get device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Create model and optimizer\n",
    "    model = DiffusionPolicy(pid_dim=6, window_size=window_size).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Create diffusion trainer\n",
    "    trainer = DiffusionTrainer(timesteps=timesteps)\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        with tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\") as pbar:\n",
    "            for batch in pbar:\n",
    "                loss = trainer.train_step(model, optimizer, batch, device)\n",
    "                epoch_losses.append(loss)\n",
    "                pbar.set_postfix({\"loss\": loss})\n",
    "        \n",
    "        # Calculate average loss\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'loss': best_loss,\n",
    "                'timesteps': timesteps,\n",
    "                'window_size': window_size,\n",
    "            }, save_path)\n",
    "            print(f\"Model saved to {save_path}\")\n",
    "    \n",
    "    return model, trainer, train_losses\n",
    "\n",
    "\n",
    "#######################\n",
    "# Evaluation Functions\n",
    "#######################\n",
    "def evaluate_pid_gains(env, pid_gains, num_episodes=5, render=False):\n",
    "    \"\"\"Evaluate PID gains on the environment.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment instance\n",
    "        pid_gains: PID gains to evaluate (normalized between -1 and 1)\n",
    "        num_episodes: Number of evaluation episodes\n",
    "        render: Whether to render the environment\n",
    "        \n",
    "    Returns:\n",
    "        metrics: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Storage for metrics\n",
    "    all_metrics = defaultdict(list)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Reset environment with different setpoints each time\n",
    "        setpoints_Cb = [0.5 + 0.5 * np.random.rand(), \n",
    "                       0.5 + 0.5 * np.random.rand()]\n",
    "        setpoints_V = [99.0 + 2.0 * np.random.rand(), \n",
    "                      99.0 + 2.0 * np.random.rand()]\n",
    "        setpoint_durations = [env.sim_steps // 2, env.sim_steps // 2 + env.sim_steps % 2]\n",
    "        \n",
    "        options = {\n",
    "            'setpoints_Cb': setpoints_Cb,\n",
    "            'setpoints_V': setpoints_V,\n",
    "            'setpoint_durations': setpoint_durations\n",
    "        }\n",
    "        \n",
    "        obs, _ = env.reset(seed=episode*100, options=options)\n",
    "        \n",
    "        # Storage for episode metrics\n",
    "        abs_Cb_errors = []\n",
    "        abs_V_errors = []\n",
    "        Cb_values = []\n",
    "        V_values = []\n",
    "        T_values = []\n",
    "        setpoint_Cb_values = []\n",
    "        setpoint_V_values = []\n",
    "        control_Tc_values = []\n",
    "        control_Fin_values = []\n",
    "        \n",
    "        # Run episode with fixed PID gains\n",
    "        done = False\n",
    "        step = 0\n",
    "        cumulative_reward = 0\n",
    "        max_overshoot_Cb = 0\n",
    "        max_overshoot_V = 0\n",
    "        \n",
    "        # For tracking setpoint changes\n",
    "        last_setpoint_Cb = None\n",
    "        last_setpoint_V = None\n",
    "        setpoint_change_steps_Cb = []\n",
    "        setpoint_change_steps_V = []\n",
    "        \n",
    "        while not done:\n",
    "            # Take action (use the same PID gains throughout the episode)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(pid_gains)\n",
    "            \n",
    "            # Get current values\n",
    "            current_Cb = info[\"true_state\"][1]\n",
    "            current_T = info[\"true_state\"][3]\n",
    "            current_V = info[\"true_state\"][4]\n",
    "            current_setpoint_Cb = info[\"setpoint_Cb\"]\n",
    "            current_setpoint_V = info[\"setpoint_V\"]\n",
    "            \n",
    "            # Track setpoint changes\n",
    "            if last_setpoint_Cb is not None and abs(current_setpoint_Cb - last_setpoint_Cb) > 0.01:\n",
    "                setpoint_change_steps_Cb.append(step)\n",
    "            if last_setpoint_V is not None and abs(current_setpoint_V - last_setpoint_V) > 0.01:\n",
    "                setpoint_change_steps_V.append(step)\n",
    "                \n",
    "            last_setpoint_Cb = current_setpoint_Cb\n",
    "            last_setpoint_V = current_setpoint_V\n",
    "            \n",
    "            # Calculate errors\n",
    "            error_Cb = current_setpoint_Cb - current_Cb\n",
    "            error_V = current_setpoint_V - current_V\n",
    "            \n",
    "            # Store values\n",
    "            abs_Cb_errors.append(abs(error_Cb))\n",
    "            abs_V_errors.append(abs(error_V))\n",
    "            Cb_values.append(current_Cb)\n",
    "            V_values.append(current_V)\n",
    "            T_values.append(current_T)\n",
    "            setpoint_Cb_values.append(current_setpoint_Cb)\n",
    "            setpoint_V_values.append(current_setpoint_V)\n",
    "            control_Tc_values.append(info[\"control_action\"][0])\n",
    "            control_Fin_values.append(info[\"control_action\"][1])\n",
    "            \n",
    "            # Calculate overshoot if we've had a setpoint change\n",
    "            if len(setpoint_change_steps_Cb) > 0:\n",
    "                # For each setpoint change\n",
    "                for change_step in setpoint_change_steps_Cb:\n",
    "                    # Only consider steps after the change\n",
    "                    if step > change_step:\n",
    "                        # Calculate overshoot relative to setpoint\n",
    "                        # Overshoot is when we go beyond the setpoint\n",
    "                        error = current_setpoint_Cb - current_Cb\n",
    "                        # If error is negative, we've overshot\n",
    "                        if error < 0:\n",
    "                            overshoot = abs(error) / current_setpoint_Cb * 100\n",
    "                            max_overshoot_Cb = max(max_overshoot_Cb, overshoot)\n",
    "            \n",
    "            # Same for volume\n",
    "            if len(setpoint_change_steps_V) > 0:\n",
    "                for change_step in setpoint_change_steps_V:\n",
    "                    if step > change_step:\n",
    "                        error = current_setpoint_V - current_V\n",
    "                        if error < 0:\n",
    "                            overshoot = abs(error) / current_setpoint_V * 100\n",
    "                            max_overshoot_V = max(max_overshoot_V, overshoot)\n",
    "            \n",
    "            # Update for next iteration\n",
    "            obs = next_obs\n",
    "            cumulative_reward += reward\n",
    "            step += 1\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Render if required\n",
    "            if render and episode == 0:\n",
    "                env.render()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        # IAE - Integral Absolute Error\n",
    "        iae_Cb = np.sum(abs_Cb_errors)\n",
    "        iae_V = np.sum(abs_V_errors)\n",
    "        \n",
    "        # ISE - Integral Squared Error\n",
    "        ise_Cb = np.sum(np.array(abs_Cb_errors)**2)\n",
    "        ise_V = np.sum(np.array(abs_V_errors)**2)\n",
    "        \n",
    "        # Mean error\n",
    "        mean_error_Cb = np.mean(abs_Cb_errors)\n",
    "        mean_error_V = np.mean(abs_V_errors)\n",
    "        \n",
    "        # Control effort (variability in control signals)\n",
    "        control_effort_Tc = np.sum(np.abs(np.diff(control_Tc_values)))\n",
    "        control_effort_Fin = np.sum(np.abs(np.diff(control_Fin_values)))\n",
    "        \n",
    "        # Rise time and settling time\n",
    "        rise_times_Cb = []\n",
    "        settling_times_Cb = []\n",
    "        rise_times_V = []\n",
    "        settling_times_V = []\n",
    "        \n",
    "        # Calculate rise and settling times for each setpoint change\n",
    "        for change_step in setpoint_change_steps_Cb:\n",
    "            if change_step + 1 < len(Cb_values):\n",
    "                # Get the new setpoint value\n",
    "                new_setpoint = setpoint_Cb_values[change_step]\n",
    "                start_value = Cb_values[change_step]\n",
    "                \n",
    "                # Calculate the change\n",
    "                total_change = new_setpoint - start_value\n",
    "                \n",
    "                # Only process significant changes\n",
    "                if abs(total_change) > 0.01:\n",
    "                    # Find rise time (time to reach 90% of the change)\n",
    "                    target_90 = start_value + 0.9 * total_change\n",
    "                    rise_time = float('inf')\n",
    "                    \n",
    "                    for i in range(change_step + 1, len(Cb_values)):\n",
    "                        if (total_change > 0 and Cb_values[i] >= target_90) or \\\n",
    "                           (total_change < 0 and Cb_values[i] <= target_90):\n",
    "                            rise_time = i - change_step\n",
    "                            break\n",
    "                            \n",
    "                    # Find settling time (time to stay within 5% of setpoint)\n",
    "                    settling_band = 0.05 * abs(total_change)\n",
    "                    settling_time = float('inf')\n",
    "                    settled = False\n",
    "                    \n",
    "                    for i in range(change_step + 1, len(Cb_values)):\n",
    "                        if abs(Cb_values[i] - new_setpoint) <= settling_band:\n",
    "                            # Check if it stays in the band\n",
    "                            if i + 5 < len(Cb_values):\n",
    "                                if all(abs(Cb_values[j] - new_setpoint) <= settling_band for j in range(i, i+5)):\n",
    "                                    settling_time = i - change_step\n",
    "                                    settled = True\n",
    "                                    break\n",
    "                    \n",
    "                    if rise_time < float('inf'):\n",
    "                        rise_times_Cb.append(rise_time)\n",
    "                    if settled:\n",
    "                        settling_times_Cb.append(settling_time)\n",
    "        \n",
    "        # Similar calculation for Volume\n",
    "        for change_step in setpoint_change_steps_V:\n",
    "            if change_step + 1 < len(V_values):\n",
    "                new_setpoint = setpoint_V_values[change_step]\n",
    "                start_value = V_values[change_step]\n",
    "                total_change = new_setpoint - start_value\n",
    "                \n",
    "                if abs(total_change) > 0.1:  # Higher threshold for volume\n",
    "                    target_90 = start_value + 0.9 * total_change\n",
    "                    rise_time = float('inf')\n",
    "                    \n",
    "                    for i in range(change_step + 1, len(V_values)):\n",
    "                        if (total_change > 0 and V_values[i] >= target_90) or \\\n",
    "                           (total_change < 0 and V_values[i] <= target_90):\n",
    "                            rise_time = i - change_step\n",
    "                            break\n",
    "                            \n",
    "                    settling_band = 0.05 * abs(total_change)\n",
    "                    settling_time = float('inf')\n",
    "                    settled = False\n",
    "                    \n",
    "                    for i in range(change_step + 1, len(V_values)):\n",
    "                        if abs(V_values[i] - new_setpoint) <= settling_band:\n",
    "                            if i + 5 < len(V_values):\n",
    "                                if all(abs(V_values[j] - new_setpoint) <= settling_band for j in range(i, i+5)):\n",
    "                                    settling_time = i - change_step\n",
    "                                    settled = True\n",
    "                                    break\n",
    "                    \n",
    "                    if rise_time < float('inf'):\n",
    "                        rise_times_V.append(rise_time)\n",
    "                    if settled:\n",
    "                        settling_times_V.append(settling_time)\n",
    "        \n",
    "        # Calculate mean rise and settling times\n",
    "        mean_rise_time_Cb = np.mean(rise_times_Cb) if rise_times_Cb else float('inf')\n",
    "        mean_settling_time_Cb = np.mean(settling_times_Cb) if settling_times_Cb else float('inf')\n",
    "        mean_rise_time_V = np.mean(rise_times_V) if rise_times_V else float('inf')\n",
    "        mean_settling_time_V = np.mean(settling_times_V) if settling_times_V else float('inf')\n",
    "        \n",
    "        # Store all metrics\n",
    "        all_metrics[\"iae_Cb\"].append(iae_Cb)\n",
    "        all_metrics[\"iae_V\"].append(iae_V)\n",
    "        all_metrics[\"ise_Cb\"].append(ise_Cb)\n",
    "        all_metrics[\"ise_V\"].append(ise_V)\n",
    "        all_metrics[\"mean_error_Cb\"].append(mean_error_Cb)\n",
    "        all_metrics[\"mean_error_V\"].append(mean_error_V)\n",
    "        all_metrics[\"control_effort_Tc\"].append(control_effort_Tc)\n",
    "        all_metrics[\"control_effort_Fin\"].append(control_effort_Fin)\n",
    "        all_metrics[\"mean_rise_time_Cb\"].append(mean_rise_time_Cb)\n",
    "        all_metrics[\"mean_settling_time_Cb\"].append(mean_settling_time_Cb)\n",
    "        all_metrics[\"mean_rise_time_V\"].append(mean_rise_time_V)\n",
    "        all_metrics[\"mean_settling_time_V\"].append(mean_settling_time_V)\n",
    "        all_metrics[\"max_overshoot_Cb\"].append(max_overshoot_Cb)\n",
    "        all_metrics[\"max_overshoot_V\"].append(max_overshoot_V)\n",
    "        all_metrics[\"cumulative_reward\"].append(cumulative_reward)\n",
    "        \n",
    "        # Store trajectories for one episode\n",
    "        if episode == 0:\n",
    "            all_metrics[\"Cb_values\"] = Cb_values\n",
    "            all_metrics[\"V_values\"] = V_values\n",
    "            all_metrics[\"T_values\"] = T_values\n",
    "            all_metrics[\"setpoint_Cb_values\"] = setpoint_Cb_values\n",
    "            all_metrics[\"setpoint_V_values\"] = setpoint_V_values\n",
    "            all_metrics[\"control_Tc_values\"] = control_Tc_values\n",
    "            all_metrics[\"control_Fin_values\"] = control_Fin_values\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_metrics = {}\n",
    "    for key, values in all_metrics.items():\n",
    "        if isinstance(values, list) and key not in [\"Cb_values\", \"V_values\", \"T_values\", \n",
    "                                                   \"setpoint_Cb_values\", \"setpoint_V_values\",\n",
    "                                                   \"control_Tc_values\", \"control_Fin_values\"]:\n",
    "            avg_metrics[key] = np.mean(values)\n",
    "    \n",
    "    # Add trajectories to avg_metrics\n",
    "    for key in [\"Cb_values\", \"V_values\", \"T_values\", \"setpoint_Cb_values\", \"setpoint_V_values\",\n",
    "               \"control_Tc_values\", \"control_Fin_values\"]:\n",
    "        if key in all_metrics:\n",
    "            avg_metrics[key] = all_metrics[key]\n",
    "    \n",
    "    return avg_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
